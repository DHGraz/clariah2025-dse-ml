{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6778d16863b961a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<!-- Title Slide -->\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"width:40%; vertical-align:middle; text-align:center;\">\n",
    "  <img src=\"https://maartengr.github.io/BERTopic/logo.png\" alt=\"Topic Modeling Illustration\" width=\"300\"/>\n",
    "</td>\n",
    "<td style=\"width:60%; vertical-align:middle; padding-left:20px;\">\n",
    "\n",
    "  # üìù Topic Modeling with BERTopic\n",
    "  ## Session: Text Embeddings\n",
    "\n",
    "  <br>\n",
    "  <br>\n",
    "  <span style=\"font-size:1.2em; color:gray;\">\n",
    "    Michael Jantscher ¬∑ TU Graz ¬∑ Know Center Research GmbH\n",
    "  </span>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1979fd664ce57d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üëã Michael Jantscher\n",
    "\n",
    "<img src=\"https://dhgraz.github.io/clariah2025-dse-ml/images/artists/profil_michael.jpg\" alt=\"Michael Jantscher\" width=\"250\"/>\n",
    "\n",
    "**PhD Student** - TU Graz <br>\n",
    "**Researcher** - Know Center Research GmbH\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Focus Areas\n",
    "- Natural Language Processing (NLP) in medical & clinical domains\n",
    "- Causal reasoning in healthcare and (neuro)radiology\n",
    "- Agentic AI systems for decision support and research workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d813d562d2c91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üìå What is a Text Embedding Vector ?- Formal Definition\n",
    "* Numerical representation of text (words, sentences or documents) in a multi-dimensional space\n",
    "* Captures meaning and context\n",
    "* Semantic similar words/sentences/documents -> vectors closer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e96c322fe22eb86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:18:06.666389Z",
     "start_time": "2025-09-05T10:18:06.656626Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".reveal .slides section { text-align: left !important; }\n",
       ".reveal h1, .reveal h2, .reveal h3, .reveal p, .reveal table { text-align: left !important; }\n",
       ".reveal table th, .reveal table td { text-align: left !important; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal .slides section { text-align: left !important; }\n",
    ".reveal h1, .reveal h2, .reveal h3, .reveal p, .reveal table { text-align: left !important; }\n",
    ".reveal table th, .reveal table td { text-align: left !important; }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bffbeba45a9191b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:19:39.427869Z",
     "start_time": "2025-09-05T10:19:32.986696Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really like this summer school!</td>\n",
       "      <td>[-0.057146158, -0.053543467, 0.045457065, 0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text  \\\n",
       "0  I really like this summer school!   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.057146158, -0.053543467, 0.045457065, 0.01...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model_small = SentenceTransformer('all-minilm-l6-v2')\n",
    "\n",
    "sample_string = \"I really like this summer school!\"\n",
    "\n",
    "sample_string_embedding = st_model_small.encode(sample_string)\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [sample_string],\n",
    "    \"embedding\": [sample_string_embedding]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50878fcfb64d0a99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üí° Uses Cases of Text Embedding Vectors\n",
    "\n",
    "| **Use Case**                   | **Example** |\n",
    "|--------------------------------|-------------|\n",
    "| **Semantic Search**         | Search ‚Äúdoctor‚Äù ‚Üí find content on ‚Äúphysicians‚Äù or ‚Äúhealthcare providers‚Äù even without exact keywords |\n",
    "| **Recommendation Systems**  | Suggest similar research papers, movies, or products based on descriptions or reviews |\n",
    "| **Sentiment Analysis**      | Understand tone (positive/negative/neutral) beyond simple keywords in tweets or reviews |\n",
    "| **Clustering & Topic Modeling** | Group thousands of news articles or support tickets by topic automatically |\n",
    "| **Chatbots & Virtual Assistants** | Improve NLU so bots answer contextually, not just by keyword |\n",
    "| **Fraud Detection**        | Spot unusual or suspicious text patterns in financial or insurance claims |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4ade64327479a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# ü¶ñ Pre-Embedding Area\n",
    "üìå **Definition:**\n",
    "Early NLP approaches represented text as **sparse, high-dimensional vectors**.\n",
    "Each dimension corresponded to a **unique word or token**, with no sense of meaning or context.\n",
    "\n",
    "---\n",
    "\n",
    "### Bag-of-Words (BoW)\n",
    "- Represents documents as a **vector of word counts**\n",
    "- Ignores grammar, order, and semantics\n",
    "- Example:\n",
    "  `\"I like NLP\"` ‚Üí `[1, 1, 1, 0, 0, ...]`\n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)\n",
    "- Adjusts raw counts to emphasize **rare, informative words** and downweight common words\n",
    "- Example: ‚Äúthe‚Äù ‚Üí low weight, ‚Äúquantum‚Äù ‚Üí high weight\n",
    "\n",
    "---\n",
    "\n",
    "#### TF-IDF Formula\n",
    "\n",
    "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times \\log\\left(\\frac{N}{DF(t)}\\right)$$\n",
    "\n",
    "Where:\n",
    "- (TF(t, d)\\): Frequency of term \\(t\\) in document \\(d\\)\n",
    "- \\(DF(t)\\): Number of documents containing \\(t\\)\n",
    "- \\(N\\): Total number of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d140f851dea732e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:25:24.141952Z",
     "start_time": "2025-09-05T10:25:23.602743Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load parlamint dataset\n",
    "df_parlamint = pd.read_csv(\"../../materials/parlamint/parlamint-it-is-2022.txt\", sep=\"\\t\")\n",
    "df_parlamint_subset = df_parlamint.head(1000).copy(deep=True)\n",
    "df_parlamint\n",
    "\n",
    "# Group sentence by utterance (=Parent_ID)\n",
    "df_parlamint_grouped = (df_parlamint.groupby([\"Parent_ID\"])[\"Text\"]\n",
    "                        .apply(lambda s: \" \".join(s))\n",
    "                        .reset_index(name=\"utterance_text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33eb4aeaadc7d2dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:22:11.632670Z",
     "start_time": "2025-09-05T10:22:11.623563Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Parent_ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg2.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>President of the United States reports:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg3.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>I have decided, according to the proposal of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg4.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Arrange sites, January 11th, 2022.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg6.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Katr√≠n Jakobsd√≥ttir's daughter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg7.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Presidential Letters for a meeting of the Gene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID                      Parent_ID  \\\n",
       "0  ParlaMint-IS_2022-01-17-20.seg2.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "1  ParlaMint-IS_2022-01-17-20.seg3.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "2  ParlaMint-IS_2022-01-17-20.seg4.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "3  ParlaMint-IS_2022-01-17-20.seg6.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "4  ParlaMint-IS_2022-01-17-20.seg7.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "\n",
       "                                                Text  \n",
       "0            President of the United States reports:  \n",
       "1  I have decided, according to the proposal of t...  \n",
       "2                 Arrange sites, January 11th, 2022.  \n",
       "3                    Katr√≠n Jakobsd√≥ttir's daughter.  \n",
       "4  Presidential Letters for a meeting of the Gene...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parlamint.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43f42ed766325e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:22:19.343624Z",
     "start_time": "2025-09-05T10:22:19.337075Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parent_ID</th>\n",
       "      <th>utterance_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>President of the United States reports: I have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u10</td>\n",
       "      <td>Before the weekend, an article by Stef√°nssonar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u11</td>\n",
       "      <td>I read this decision in Perconte, which is not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u12</td>\n",
       "      <td>In fact, this is shown in the letter quoted by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u13</td>\n",
       "      <td>Yes, that's right. That's right. A senator who...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Parent_ID  \\\n",
       "0   ParlaMint-IS_2022-01-17-20.u1   \n",
       "1  ParlaMint-IS_2022-01-17-20.u10   \n",
       "2  ParlaMint-IS_2022-01-17-20.u11   \n",
       "3  ParlaMint-IS_2022-01-17-20.u12   \n",
       "4  ParlaMint-IS_2022-01-17-20.u13   \n",
       "\n",
       "                                      utterance_text  \n",
       "0  President of the United States reports: I have...  \n",
       "1  Before the weekend, an article by Stef√°nssonar...  \n",
       "2  I read this decision in Perconte, which is not...  \n",
       "3  In fact, this is shown in the letter quoted by...  \n",
       "4  Yes, that's right. That's right. A senator who...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parlamint_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "340e635d8fcb036b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:29:00.492119Z",
     "start_time": "2025-09-05T10:29:00.474252Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President of the United States reports:\n",
      "I have decided, according to the proposal of the prime minister, that the Council should meet for an extended meeting on Monday, January 17, 2022 p.m. 3:00.\n",
      "Arrange sites, January 11th, 2022.\n",
      "Katr√≠n Jakobsd√≥ttir's daughter.\n",
      "Presidential Letters for a meeting of the General Assembly for a subsequent meeting on January 17, 2022\n",
      "I'd like to use this opportunity here after reading this letter and offer the highest. President and w. Senators welcome to New Year's Parliamentary Conferences.\n"
     ]
    }
   ],
   "source": [
    "# Take a single utterance from the dataset\n",
    "sample_utterance = df_parlamint[df_parlamint[\"Parent_ID\"] == \"ParlaMint-IS_2022-01-17-20.u1\"][\"Text\"]\n",
    "print(\"\\n\".join([e for e in sample_utterance]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9385afaac9900eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:32:56.073488Z",
     "start_time": "2025-09-05T10:32:56.061565Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 'transform' only...\n",
      "Number features: 5 ['17 2022' '2022' 'january' 'january 17' 'meeting']\n",
      "Shape embedding array: (6, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17 2022</th>\n",
       "      <th>2022</th>\n",
       "      <th>january</th>\n",
       "      <th>january 17</th>\n",
       "      <th>meeting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   17 2022  2022  january  january 17  meeting\n",
       "0        0     0        0           0        0\n",
       "1        1     1        1           1        1\n",
       "2        0     1        1           0        0\n",
       "3        0     0        0           0        0\n",
       "4        1     1        1           1        2\n",
       "5        0     0        0           0        0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.models import CountVectorizerEmbedder\n",
    "\n",
    "# Adding just the utterance sample as vocabulary\n",
    "cv_model = CountVectorizerEmbedder(vocabulary=sample_utterance, max_features=5, stop_words='english',\n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "cv_embeddings = cv_model.embed(sample_utterance)\n",
    "print(f\"Number features: {len(cv_model.embedding_model.get_feature_names_out())}\", cv_model.embedding_model.get_feature_names_out())\n",
    "print(f\"Shape embedding array: {cv_embeddings.toarray().shape}\")\n",
    "df_cv_output = pd.DataFrame(columns=cv_model.embedding_model.get_feature_names_out(), data=cv_embeddings.toarray())\n",
    "df_cv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b168738213bc1973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:32:24.989378Z",
     "start_time": "2025-09-05T10:32:24.978760Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 'transform' only...\n",
      "Number features: 5 ['17' '2022' 'january' 'meeting' 'president']\n",
      "Shape embedding array: (6, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17</th>\n",
       "      <th>2022</th>\n",
       "      <th>january</th>\n",
       "      <th>meeting</th>\n",
       "      <th>president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.540298</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.540298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.394497</td>\n",
       "      <td>0.333062</td>\n",
       "      <td>0.333062</td>\n",
       "      <td>0.788994</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         17      2022   january   meeting  president\n",
       "0  0.000000  0.000000  0.000000  0.000000        1.0\n",
       "1  0.540298  0.456156  0.456156  0.540298        0.0\n",
       "2  0.000000  0.707107  0.707107  0.000000        0.0\n",
       "3  0.000000  0.000000  0.000000  0.000000        0.0\n",
       "4  0.394497  0.333062  0.333062  0.788994        0.0\n",
       "5  0.000000  0.000000  0.000000  0.000000        1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.models import TfIdfEmbedder\n",
    "\n",
    "# Adding just the utterance sample as vocabulary\n",
    "tfidf_model = TfIdfEmbedder(vocabulary=sample_utterance, max_features=5, stop_words='english')\n",
    "tfidf_embeddings = tfidf_model.embed(sample_utterance)\n",
    "print(f\"Number features: {len(tfidf_model.embedding_model.get_feature_names_out())}\", tfidf_model.embedding_model.get_feature_names_out())\n",
    "print(f\"Shape embedding array: {tfidf_embeddings.toarray().shape}\")\n",
    "df_tfidf_output = pd.DataFrame(columns=tfidf_model.embedding_model.get_feature_names_out(), data=tfidf_embeddings.toarray())\n",
    "df_tfidf_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0f058c66c231d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dense Text Embeddings\n",
    "\n",
    "üìå **Definition:**\n",
    "Dense embeddings represent words, sentences, or documents as **low-dimensional, dense vectors**\n",
    "where similar meanings are **close together in vector space**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Characteristics\n",
    "- **Low-dimensional** (e.g., 100‚Äì1,536 dimensions, not vocab-sized)\n",
    "- **Dense representation**: most values ‚â† 0\n",
    "- Captures **semantic meaning** and context\n",
    "- Learned from data via **neural networks**\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Brief History\n",
    "- **Word2Vec (2013)** ‚Äì First widely used dense word embeddings (Mikolov et al.)\n",
    "- **GloVe (2014)** ‚Äì Global Vectors for word representation\n",
    "- **FastText (2016)** ‚Äì Adds subword information for better handling of rare words\n",
    "- **ELMo (2018)** ‚Äì Contextual word embeddings\n",
    "- **BERT (2018)** ‚Äì Contextual embeddings for entire sentences\n",
    "- **OpenAI / Modern Embeddings (2020s)** ‚Äì High-quality sentence/document embeddings (e.g., `text-embedding-3-large`)\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Why It‚Äôs Better\n",
    "- Reduces dimensionality dramatically\n",
    "- Learns **semantic relationships**\n",
    "- Powers modern **search, recommendation, and AI assistants**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9a28e28a13c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:59:22.623450Z",
     "start_time": "2025-09-05T10:59:22.617960Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word2Vec: CBOW & Skip-Gram Overview\n",
    "\n",
    "### üü¢ What is Word2Vec?\n",
    "- A **shallow, two-layer neural network** that learns word embeddings from context.\n",
    "- Maps words to **dense vectors** in a continuous space; similar words are **close together**.\n",
    "- Introduced by Mikolov et al., **2013**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Architectures\n",
    "\n",
    "#### üîπ Continuous Bag-of-Words (CBOW)\n",
    "- Predicts the **center word** given its context words.\n",
    "- Fast to train; works well for **frequent words**.\n",
    "\n",
    "#### üîπ Skip-Gram\n",
    "- Predicts **context words** given a center word.\n",
    "- Performs better for **rare words**, large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Why It Matters\n",
    "- Captures **semantic relationships**:\n",
    "  `king - man + woman ‚âà queen`\n",
    "- Major leap from sparse (BoW/TF-IDF) to **dense, meaningful embeddings**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532683a1f125d49d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word2Vec: Training & Visual Intuition\n",
    "\n",
    "### üü¢ Training Workflow\n",
    "1. **One-hot encoding** for words.\n",
    "2. **Hidden layer** = embedding lookup table.\n",
    "3. **Output layer** predicts context words (softmax with negative sampling).\n",
    "4. Final **hidden layer weights** = embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Visual Intuition\n",
    "\n",
    "<img src=\"https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/word2vec_diagrams.png\" alt=\"Word2Vec Architecture\" style=\"width:40%;\">\n",
    "\n",
    "- Diagram shows the Skip-Gram model predicting context words.\n",
    "- Only the **embedding layer weights** are retained.\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Israel G. (2017). *Word2Vec Explained*.\n",
    "  [https://israelg99.github.io/2017-03-23-Word2Vec-Explained/](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1a24c6a4ca3de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT: Contextual Embeddings\n",
    "\n",
    "### üü¢ What is BERT?\n",
    "- **Bidirectional Encoder Representations from Transformers** (2018, Google AI).\n",
    "- Uses **Transformer architecture** to create **contextual word embeddings**:\n",
    "  - Each word‚Äôs vector depends on **all surrounding words** (left & right context).\n",
    "- Trained on **masked language modeling** and **next sentence prediction** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Key Innovations\n",
    "- **Bidirectional**: Unlike Word2Vec/Glove, captures context from both sides.\n",
    "- **Transformer encoder layers** with self-attention results in rich, deep embeddings.\n",
    "- **Contextualization**: Same word gets **different vectors** depending on context\n",
    "  (‚Äúbank‚Äù in ‚Äúriver bank‚Äù vs. ‚Äúbank account‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Visual Intuition\n",
    "\n",
    "<img src=\"images/bert_embeddings.png\" alt=\"BERT Transformer\" style=\"width:40%;\">\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Devlin et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.*\n",
    "  [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b858a91d945d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##  Word2Vec: Learning Word Embeddings with CBOW & Skip-Gram\n",
    "\n",
    "###  What is Word2Vec?\n",
    "- A **shallow, two-layer neural network** model that learns word embeddings by reconstructing linguistic context‚Äîmapping each word to a vector in a continuous, semantic space. Words with similar contexts end up **close together**. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "---\n",
    "\n",
    "###  Model Architectures\n",
    "\n",
    "#### **Continuous Bag-of-Words (CBOW)**\n",
    "- **Predicts** a target word using its surrounding context words.\n",
    "- The model **averages** context word vectors (from a fixed-size window) to forecast the center word. :contentReference[oaicite:2]{index=2}\n",
    "- **Faster to train**, especially effective for **frequent words**.\n",
    "\n",
    "#### **Skip-Gram**\n",
    "- **Predicts** surrounding context words based on a target (center) word.\n",
    "- Treats each context-target pair as a separate training example‚Äîespecially effective for **rare words** and with **large corpora**. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "---\n",
    "\n",
    "###  Training Workflow & Embedding Extraction\n",
    "1. Input words are one-hot encoded.\n",
    "2. A hidden layer serves as the embedding lookup (weight matrix).\n",
    "3. Output layer produces probability distributions‚Äîusing **softmax** or efficient approximations like **negative sampling** or **hierarchical softmax**. :contentReference[oaicite:4]{index=4}\n",
    "4. After training, the **hidden layer weights** become the final word embeddings for use in downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why It Matters\n",
    "- Captures rich **semantic and syntactic relationships**:\n",
    "  \\( \\text{\"king\"} - \\text{\"man\"} + \\text{\"woman\"} \\approx \\text{\"queen\"} \\). :contentReference[oaicite:5]{index=5}\n",
    "- A major leap from sparse, count-based methods‚ÄîWord2Vec made dense, meaningful text representations both practical and scalable. Word2Vec was first published in **2013**. :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary Table\n",
    "\n",
    "| Architecture | Strengths | Best For |\n",
    "|--------------|-----------|----------|\n",
    "| **CBOW**     | Fast training, efficient, good for frequent words | Large datasets where speed matters |\n",
    "| **Skip-Gram**| Robust with rare words, better embedding granularity | Large corpus with focus on subtle word nuances |\n",
    "\n",
    "---\n",
    "\n",
    "###  Visual Intuition\n",
    "![Word2Vec Diagram](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/word2vec_diagrams.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Israel G. (2017). *Word2Vec Explained* ‚Äî [https://israelg99.github.io/2017-03-23-Word2Vec-Explained/](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fec943935de725",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Why Neural (Dense) Embeddings over Sparse Vector Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe11323e9bd312",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Further topics to cover in the lecture\n",
    "* Chunking is very important!\n",
    "* How does training look like for the different models?\n",
    "\n",
    "### Select different sentence-transformer embedding models\n",
    "* https://huggingface.co/models?pipeline_tag=sentence-similarity\n",
    "\n",
    "All generated embeddings are based on [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) <br>\n",
    "Maybe give a smaller model also a try: [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "### Training\n",
    "* How could a training look like?\n",
    "* Which kind of data is necessary therefore?\n",
    "* Differences between sparse and dense models in terms of training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee13a77a5d1c07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Interesting Tasks\n",
    "* Find other promising embedding models (sparse and dense) and try them out\n",
    "    * What about Doc2Vec, word2vec, bag-of-words etc [here](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html)\n",
    "* Calculate the embedding similarities like in [here](https://huggingface.co/sentence-transformers)\n",
    "    * There might be different similarity scores dependeing on the embedder (sparse vs dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1089d984f122eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:23:27.633817Z",
     "start_time": "2025-09-04T11:23:27.627398Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
