{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6778d16863b961a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<!-- Title Slide -->\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"width:40%; vertical-align:middle; text-align:center;\">\n",
    "  <img src=\"https://maartengr.github.io/BERTopic/logo.png\" alt=\"Topic Modeling Illustration\" width=\"300\"/>\n",
    "</td>\n",
    "<td style=\"width:60%; vertical-align:middle; padding-left:20px;\">\n",
    "\n",
    "  # üìù Topic Modeling with BERTopic\n",
    "  ## Session: Text Embeddings\n",
    "\n",
    "  <br>\n",
    "  <br>\n",
    "  <span style=\"font-size:1.2em; color:gray;\">\n",
    "    Michael Jantscher ¬∑ TU Graz ¬∑ Know Center Research GmbH\n",
    "  </span>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1979fd664ce57d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üëã Michael Jantscher\n",
    "\n",
    "<img src=\"https://dhgraz.github.io/clariah2025-dse-ml/images/artists/profil_michael.jpg\" alt=\"Michael Jantscher\" width=\"250\"/>\n",
    "\n",
    "**PhD Student** - TU Graz <br>\n",
    "**Researcher** - Know Center Research GmbH\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Focus Areas\n",
    "- Natural Language Processing (NLP) in medical & clinical domains\n",
    "- Causal reasoning in healthcare and (neuro)radiology\n",
    "- Agentic AI systems for decision support and research workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d813d562d2c91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üìå What is a Text Embedding Vector ?- Formal Definition\n",
    "* Numerical representation of text (words, sentences or documents) in a multi-dimensional space\n",
    "* Captures meaning and context\n",
    "* Semantic similar words/sentences/documents -> vectors closer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e96c322fe22eb86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:18:06.666389Z",
     "start_time": "2025-09-05T10:18:06.656626Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".reveal .slides section { text-align: left !important; }\n",
       ".reveal h1, .reveal h2, .reveal h3, .reveal p, .reveal table { text-align: left !important; }\n",
       ".reveal table th, .reveal table td { text-align: left !important; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal .slides section { text-align: left !important; }\n",
    ".reveal h1, .reveal h2, .reveal h3, .reveal p, .reveal table { text-align: left !important; }\n",
    ".reveal table th, .reveal table td { text-align: left !important; }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bffbeba45a9191b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:19:39.427869Z",
     "start_time": "2025-09-05T10:19:32.986696Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really like this summer school!</td>\n",
       "      <td>[-0.057146158, -0.053543467, 0.045457065, 0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text  \\\n",
       "0  I really like this summer school!   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.057146158, -0.053543467, 0.045457065, 0.01...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model_small = SentenceTransformer('all-minilm-l6-v2')\n",
    "\n",
    "sample_string = \"I really like this summer school!\"\n",
    "\n",
    "sample_string_embedding = st_model_small.encode(sample_string)\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [sample_string],\n",
    "    \"embedding\": [sample_string_embedding]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50878fcfb64d0a99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üí° Uses Cases of Text Embedding Vectors\n",
    "\n",
    "| **Use Case**                   | **Example** |\n",
    "|--------------------------------|-------------|\n",
    "| **Semantic Search**         | Search ‚Äúdoctor‚Äù ‚Üí find content on ‚Äúphysicians‚Äù or ‚Äúhealthcare providers‚Äù even without exact keywords |\n",
    "| **Recommendation Systems**  | Suggest similar research papers, movies, or products based on descriptions or reviews |\n",
    "| **Sentiment Analysis**      | Understand tone (positive/negative/neutral) beyond simple keywords in tweets or reviews |\n",
    "| **Clustering & Topic Modeling** | Group thousands of news articles or support tickets by topic automatically |\n",
    "| **Chatbots & Virtual Assistants** | Improve NLU so bots answer contextually, not just by keyword |\n",
    "| **Fraud Detection**        | Spot unusual or suspicious text patterns in financial or insurance claims |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4ade64327479a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# ü¶ñ Pre-Embedding Area\n",
    "üìå **Definition:**\n",
    "Early NLP approaches represented text as **sparse, high-dimensional vectors**.\n",
    "Each dimension corresponded to a **unique word or token**, with no sense of meaning or context.\n",
    "\n",
    "---\n",
    "\n",
    "### Bag-of-Words (BoW)\n",
    "- Represents documents as a **vector of word counts**\n",
    "- Ignores grammar, order, and semantics\n",
    "- Example:\n",
    "  `\"I like NLP\"` ‚Üí `[1, 1, 1, 0, 0, ...]`\n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)\n",
    "- Adjusts raw counts to emphasize **rare, informative words** and downweight common words\n",
    "- Example: ‚Äúthe‚Äù ‚Üí low weight, ‚Äúquantum‚Äù ‚Üí high weight\n",
    "\n",
    "---\n",
    "\n",
    "#### TF-IDF Formula\n",
    "\n",
    "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times \\log\\left(\\frac{N}{DF(t)}\\right)$$\n",
    "\n",
    "Where:\n",
    "- (TF(t, d)\\): Frequency of term \\(t\\) in document \\(d\\)\n",
    "- \\(DF(t)\\): Number of documents containing \\(t\\)\n",
    "- \\(N\\): Total number of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d140f851dea732e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:25:24.141952Z",
     "start_time": "2025-09-05T10:25:23.602743Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load parlamint dataset\n",
    "df_parlamint = pd.read_csv(\"../../materials/parlamint/parlamint-it-is-2022.txt\", sep=\"\\t\")\n",
    "df_parlamint_subset = df_parlamint.head(1000).copy(deep=True)\n",
    "df_parlamint\n",
    "\n",
    "# Group sentence by utterance (=Parent_ID)\n",
    "df_parlamint_grouped = (df_parlamint.groupby([\"Parent_ID\"])[\"Text\"]\n",
    "                        .apply(lambda s: \" \".join(s))\n",
    "                        .reset_index(name=\"utterance_text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33eb4aeaadc7d2dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:22:11.632670Z",
     "start_time": "2025-09-05T10:22:11.623563Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Parent_ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg2.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>President of the United States reports:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg3.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>I have decided, according to the proposal of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg4.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Arrange sites, January 11th, 2022.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg6.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Katr√≠n Jakobsd√≥ttir's daughter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg7.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Presidential Letters for a meeting of the Gene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID                      Parent_ID  \\\n",
       "0  ParlaMint-IS_2022-01-17-20.seg2.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "1  ParlaMint-IS_2022-01-17-20.seg3.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "2  ParlaMint-IS_2022-01-17-20.seg4.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "3  ParlaMint-IS_2022-01-17-20.seg6.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "4  ParlaMint-IS_2022-01-17-20.seg7.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "\n",
       "                                                Text  \n",
       "0            President of the United States reports:  \n",
       "1  I have decided, according to the proposal of t...  \n",
       "2                 Arrange sites, January 11th, 2022.  \n",
       "3                    Katr√≠n Jakobsd√≥ttir's daughter.  \n",
       "4  Presidential Letters for a meeting of the Gene...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parlamint.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43f42ed766325e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:22:19.343624Z",
     "start_time": "2025-09-05T10:22:19.337075Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parent_ID</th>\n",
       "      <th>utterance_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>President of the United States reports: I have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u10</td>\n",
       "      <td>Before the weekend, an article by Stef√°nssonar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u11</td>\n",
       "      <td>I read this decision in Perconte, which is not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u12</td>\n",
       "      <td>In fact, this is shown in the letter quoted by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u13</td>\n",
       "      <td>Yes, that's right. That's right. A senator who...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Parent_ID  \\\n",
       "0   ParlaMint-IS_2022-01-17-20.u1   \n",
       "1  ParlaMint-IS_2022-01-17-20.u10   \n",
       "2  ParlaMint-IS_2022-01-17-20.u11   \n",
       "3  ParlaMint-IS_2022-01-17-20.u12   \n",
       "4  ParlaMint-IS_2022-01-17-20.u13   \n",
       "\n",
       "                                      utterance_text  \n",
       "0  President of the United States reports: I have...  \n",
       "1  Before the weekend, an article by Stef√°nssonar...  \n",
       "2  I read this decision in Perconte, which is not...  \n",
       "3  In fact, this is shown in the letter quoted by...  \n",
       "4  Yes, that's right. That's right. A senator who...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parlamint_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "340e635d8fcb036b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:29:00.492119Z",
     "start_time": "2025-09-05T10:29:00.474252Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President of the United States reports:\n",
      "I have decided, according to the proposal of the prime minister, that the Council should meet for an extended meeting on Monday, January 17, 2022 p.m. 3:00.\n",
      "Arrange sites, January 11th, 2022.\n",
      "Katr√≠n Jakobsd√≥ttir's daughter.\n",
      "Presidential Letters for a meeting of the General Assembly for a subsequent meeting on January 17, 2022\n",
      "I'd like to use this opportunity here after reading this letter and offer the highest. President and w. Senators welcome to New Year's Parliamentary Conferences.\n"
     ]
    }
   ],
   "source": [
    "# Take a single utterance from the dataset\n",
    "sample_utterance = df_parlamint[df_parlamint[\"Parent_ID\"] == \"ParlaMint-IS_2022-01-17-20.u1\"][\"Text\"]\n",
    "print(\"\\n\".join([e for e in sample_utterance]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9385afaac9900eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:32:56.073488Z",
     "start_time": "2025-09-05T10:32:56.061565Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 'transform' only...\n",
      "Number features: 5 ['17 2022' '2022' 'january' 'january 17' 'meeting']\n",
      "Shape embedding array: (6, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17 2022</th>\n",
       "      <th>2022</th>\n",
       "      <th>january</th>\n",
       "      <th>january 17</th>\n",
       "      <th>meeting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   17 2022  2022  january  january 17  meeting\n",
       "0        0     0        0           0        0\n",
       "1        1     1        1           1        1\n",
       "2        0     1        1           0        0\n",
       "3        0     0        0           0        0\n",
       "4        1     1        1           1        2\n",
       "5        0     0        0           0        0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.models import CountVectorizerEmbedder\n",
    "\n",
    "# Adding just the utterance sample as vocabulary\n",
    "cv_model = CountVectorizerEmbedder(vocabulary=sample_utterance, max_features=5, stop_words='english',\n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "cv_embeddings = cv_model.embed(sample_utterance)\n",
    "print(f\"Number features: {len(cv_model.embedding_model.get_feature_names_out())}\", cv_model.embedding_model.get_feature_names_out())\n",
    "print(f\"Shape embedding array: {cv_embeddings.toarray().shape}\")\n",
    "df_cv_output = pd.DataFrame(columns=cv_model.embedding_model.get_feature_names_out(), data=cv_embeddings.toarray())\n",
    "df_cv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b168738213bc1973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:32:24.989378Z",
     "start_time": "2025-09-05T10:32:24.978760Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 'transform' only...\n",
      "Number features: 5 ['17' '2022' 'january' 'meeting' 'president']\n",
      "Shape embedding array: (6, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17</th>\n",
       "      <th>2022</th>\n",
       "      <th>january</th>\n",
       "      <th>meeting</th>\n",
       "      <th>president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.540298</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.540298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.394497</td>\n",
       "      <td>0.333062</td>\n",
       "      <td>0.333062</td>\n",
       "      <td>0.788994</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         17      2022   january   meeting  president\n",
       "0  0.000000  0.000000  0.000000  0.000000        1.0\n",
       "1  0.540298  0.456156  0.456156  0.540298        0.0\n",
       "2  0.000000  0.707107  0.707107  0.000000        0.0\n",
       "3  0.000000  0.000000  0.000000  0.000000        0.0\n",
       "4  0.394497  0.333062  0.333062  0.788994        0.0\n",
       "5  0.000000  0.000000  0.000000  0.000000        1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.models import TfIdfEmbedder\n",
    "\n",
    "# Adding just the utterance sample as vocabulary\n",
    "tfidf_model = TfIdfEmbedder(vocabulary=sample_utterance, max_features=5, stop_words='english')\n",
    "tfidf_embeddings = tfidf_model.embed(sample_utterance)\n",
    "print(f\"Number features: {len(tfidf_model.embedding_model.get_feature_names_out())}\", tfidf_model.embedding_model.get_feature_names_out())\n",
    "print(f\"Shape embedding array: {tfidf_embeddings.toarray().shape}\")\n",
    "df_tfidf_output = pd.DataFrame(columns=tfidf_model.embedding_model.get_feature_names_out(), data=tfidf_embeddings.toarray())\n",
    "df_tfidf_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0f058c66c231d",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dense Text Embeddings\n",
    "\n",
    "üìå **Definition:**\n",
    "Dense embeddings represent words, sentences, or documents as **low-dimensional, dense vectors**\n",
    "where similar meanings are **close together in vector space**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Characteristics\n",
    "- **Low-dimensional** (e.g., 100‚Äì1,536 dimensions, not vocab-sized)\n",
    "- **Dense representation**: most values ‚â† 0\n",
    "- Captures **semantic meaning** and context\n",
    "- Learned from data via **neural networks**\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Brief History\n",
    "- **Word2Vec (2013)** ‚Äì First widely used dense word embeddings (Mikolov et al.)\n",
    "- **GloVe (2014)** ‚Äì Global Vectors for word representation\n",
    "- **FastText (2016)** ‚Äì Adds subword information for better handling of rare words\n",
    "- **ELMo (2018)** ‚Äì Contextual word embeddings\n",
    "- **BERT (2018)** ‚Äì Contextual embeddings for entire sentences\n",
    "- **OpenAI / Modern Embeddings (2020s)** ‚Äì High-quality sentence/document embeddings (e.g., `text-embedding-3-large`)\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Why It‚Äôs Better\n",
    "- Reduces dimensionality dramatically\n",
    "- Learns **semantic relationships**\n",
    "- Powers modern **search, recommendation, and AI assistants**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9a28e28a13c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:59:22.623450Z",
     "start_time": "2025-09-05T10:59:22.617960Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word2Vec: CBOW & Skip-Gram Overview\n",
    "\n",
    "### üü¢ What is Word2Vec?\n",
    "- A **shallow, two-layer neural network** that learns word embeddings from context.\n",
    "- Maps words to **dense vectors** in a continuous space; similar words are **close together**.\n",
    "- Introduced by Mikolov et al., **2013**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Architectures\n",
    "\n",
    "#### üîπ Continuous Bag-of-Words (CBOW)\n",
    "- Predicts the **center word** given its context words.\n",
    "- Fast to train; works well for **frequent words**.\n",
    "\n",
    "#### üîπ Skip-Gram\n",
    "- Predicts **context words** given a center word.\n",
    "- Performs better for **rare words**, large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Why It Matters\n",
    "- Captures **semantic relationships**:\n",
    "  `king - man + woman ‚âà queen`\n",
    "- Major leap from sparse (BoW/TF-IDF) to **dense, meaningful embeddings**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532683a1f125d49d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word2Vec: Training & Visual Intuition\n",
    "\n",
    "### üü¢ Training Workflow\n",
    "1. **One-hot encoding** for words.\n",
    "2. **Hidden layer** = embedding lookup table.\n",
    "3. **Output layer** predicts context words (softmax with negative sampling).\n",
    "4. Final **hidden layer weights** = embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Visual Intuition\n",
    "\n",
    "<img src=\"https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/word2vec_diagrams.png\" alt=\"Word2Vec Architecture\" style=\"width:40%;\">\n",
    "\n",
    "- Diagram shows the Skip-Gram model predicting context words.\n",
    "- Only the **embedding layer weights** are retained.\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Israel G. (2017). *Word2Vec Explained*.\n",
    "  [https://israelg99.github.io/2017-03-23-Word2Vec-Explained/](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1a24c6a4ca3de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT: Contextual Embeddings\n",
    "\n",
    "### üü¢ What is BERT?\n",
    "- **Bidirectional Encoder Representations from Transformers** (2018, Google AI).\n",
    "- Uses **Transformer architecture** to create **contextual word embeddings**:\n",
    "  - Each word‚Äôs vector depends on **all surrounding words** (left & right context).\n",
    "- Trained on **masked language modeling** and **next sentence prediction** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Key Innovations\n",
    "- **Bidirectional**: Unlike Word2Vec/Glove, captures context from both sides.\n",
    "- **Transformer encoder layers** with self-attention results in rich, deep embeddings.\n",
    "- **Contextualization**: Same word gets **different vectors** depending on context\n",
    "  (‚Äúbank‚Äù in ‚Äúriver bank‚Äù vs. ‚Äúbank account‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Visual Intuition\n",
    "\n",
    "<img src=\"images/bert_embeddings.png\" alt=\"BERT Transformer\" style=\"width:40%;\">\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Devlin et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.*\n",
    "  [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80320a1691db2328",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Sentence Transformers Recap & Training\n",
    "\n",
    "### üü¢ Key Takeaways\n",
    "- **Sparse vectors** (BoW, TF-IDF):\n",
    "  - One dimension per word, mostly zeros\n",
    "  - No deep semantic meaning\n",
    "- **Dense embeddings** (Word2Vec, BERT, SBERT):\n",
    "  - Low-dimensional, rich semantic context\n",
    "  - Words/sentences cluster by meaning\n",
    "\n",
    "---\n",
    "\n",
    "### üü† How SBERT is Trained\n",
    "- **Backbone:** Pretrained BERT or RoBERTa encoders\n",
    "- **Siamese/Triplet Network Architecture:**\n",
    "  - Encodes two or three sentences **independently** into embeddings\n",
    "  - Trains to minimize distance for similar sentences and maximize for dissimilar ones\n",
    "- **Training Objectives [Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html):**\n",
    "  - **Contrastive loss** (distance-based similarity)\n",
    "  - **Natural Language Inference** datasets (entailment, contradiction, neutral)\n",
    "  - **MultipleNegativesRankingLoss** for retrieval tasks\n",
    "- **Result:**\n",
    "  - Embeddings suitable for **cosine similarity** ‚Üí semantic search, clustering, recommendations\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Reimers & Gurevych (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*\n",
    "  [https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)\n",
    "- SentenceTransformers Documentation [https://www.sbert.net/](https://www.sbert.net/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c076afcd42b56e09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T12:06:53.325241Z",
     "start_time": "2025-09-05T12:06:49.768094Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really like this summer school!</td>\n",
       "      <td>[-0.057146158, -0.053543467, 0.045457065, 0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text  \\\n",
       "0  I really like this summer school!   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.057146158, -0.053543467, 0.045457065, 0.01...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model_small = SentenceTransformer('all-minilm-l6-v2')\n",
    "\n",
    "sample_string = \"I really like this summer school!\"\n",
    "\n",
    "sample_string_embedding = st_model_small.encode(sample_string)\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [sample_string],\n",
    "    \"embedding\": [sample_string_embedding]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ce1437f2cd776",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Distance & Similarity Measures (Brief Intro)\n",
    "\n",
    "### üü¢ Why It Matters\n",
    "- Compare embeddings ‚Üí find **semantic similarity** between words, sentences, or documents.\n",
    "- Core to **semantic search, clustering, and topic modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Common Measures\n",
    "\n",
    "- üîπ **Dot Product**\n",
    "  Unnormalized similarity; sensitive to magnitude:\n",
    "  $$\n",
    "  a \\cdot b = \\sum a_i b_i\n",
    "  $$\n",
    "\n",
    "- üîπ **Cosine Similarity**\n",
    "  Measures **angle** between vectors (ignores magnitude):\n",
    "  $$\n",
    "  \\text{cosine\\_sim}(a, b) = \\frac{a \\cdot b}{\\|a\\|\\|b\\|}\n",
    "  $$\n",
    "\n",
    "- üîπ **Euclidean Distance**\n",
    "  Straight-line distance in vector space:\n",
    "  $$\n",
    "  d(a, b) = \\sqrt{\\sum (a_i - b_i)^2}\n",
    "  $$\n",
    "\n",
    "- üîπ **Manhattan (L1) Distance**\n",
    "  Sum of absolute differences:\n",
    "  $$\n",
    "  d_{\\text{L1}}(a, b) = \\sum |a_i - b_i|\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060a9ae18f92011",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Chunking Techniques for Embeddings\n",
    "\n",
    "### üü¢ Why Chunking?\n",
    "- Long documents exceed model token limits (e.g., BERT ~512 tokens).\n",
    "- Splitting text into **manageable chunks** improves:\n",
    "  - ‚úÖ Embedding quality\n",
    "  - ‚úÖ Retrieval accuracy\n",
    "  - ‚úÖ Context management in downstream tasks\n",
    "- üéØ **Goal of Good Chunking:**\n",
    "  Create chunks that are **small enough** to fit model limits\n",
    "  but **large enough** to retain full semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Common Techniques\n",
    "\n",
    "1. **Fixed-Length Chunking**\n",
    "   - Split text into chunks of `N` tokens/words.\n",
    "   - Simple, fast, but can cut off sentences mid-way.\n",
    "\n",
    "2. **Sentence-Based Chunking**\n",
    "   - Split by sentence boundaries (NLTK, spaCy).\n",
    "   - Better for readability, semantic grouping.\n",
    "\n",
    "3. **Paragraph-Based Chunking**\n",
    "   - Keep natural paragraph structure.\n",
    "   - Good for preserving context, but chunk sizes vary.\n",
    "\n",
    "4. **Sliding Window / Overlapping Chunks**\n",
    "   - Add overlap between chunks (e.g., 50 tokens).\n",
    "   - Prevents loss of context between splits.\n",
    "\n",
    "5. **Semantic Chunking**\n",
    "   - Use topic segmentation or embeddings to find boundaries.\n",
    "   - Most accurate, but computationally heavier.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Choose technique based on document length, model limits, and retrieval needs.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860bdd8b5a60922",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": "# üèãÔ∏è Exercise: Simple Embedding Use Case"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c99b05779465c099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T12:12:19.218339Z",
     "start_time": "2025-09-05T12:11:32.456584Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 432/432 [00:10<00:00, 40.07it/s] \n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5018/5018 [00:33<00:00, 148.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode utterance-wise dataset\n",
    "df_parlamint_embeddings_per_utterance = st_model_small.encode(df_parlamint_grouped[\"utterance_text\"].to_list(),\n",
    "                                                     show_progress_bar=True)\n",
    "\n",
    "# Encode sentence-wise dataset\n",
    "df_parlamint_embeddings_per_sentence = st_model_small.encode(df_parlamint[\"Text\"].to_list(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fa2bd617d780a64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T12:13:32.275519Z",
     "start_time": "2025-09-05T12:13:32.234989Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parlamint_grouped[\"embedding\"] = list(df_parlamint_embeddings_per_utterance)\n",
    "df_parlamint[\"embedding\"] = list(df_parlamint_embeddings_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f24c2bd73106548e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T12:16:35.013067Z",
     "start_time": "2025-09-05T12:16:34.980701Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5964 | Utterance: After listening to the highest. Minister, both today and yesterday, I get a little bit of the feeling that he looks at himself and his Ministry more like an observer than a doer when it comes to reducing greenhouse gas emissions. It is best that Ministers burn for more and larger activations, but, as the energy manager has noted, the energy flows directly into the energy exchange. It takes a very clear policy, but it needs a whole plan to make sure it does. That's why it hurts to the top. Ministers will not give us very clear answers on how Iceland's national target of climate change will be updated, when it will happen, and whether the government's climate programme of action will be reviewed and how these updated targets will appear in government policy and measures at all times. But I hope it reaches the highest. Minister to review it better afterwards. Last night we talked about the bus and the electric car truck. It turned out that the government still assumes that fossil fuels are being brought into the country until 2030. In the year 2040, there are still cars that remain undiscovered, yet the government's treaties show that by the year 2040 Iceland is to be free of fossil fuels. I'm going to quote the highest. Minister, with permission of presidents: If the government's current objective is to achieve better results, I would be the first to celebrate it, and then we can reevaluate our plans. Are these the words of a politician burning for bringing Iceland into the green future by a revolution in public transport and a massive plan for the recovery of wetlands? No, these are the words of someone who's going to sit there hoping things would get tough, following the line of events, as the government's policy on climate change has been. What we need in the climate Ministry is a fighter or fighter for radical climate operations, which can activate the solidarity of young people and older ones in real action in behalf of climate, earth, and all of us and future generations.\n",
      "\n",
      "Score: 0.5897 | Utterance: I'll mention this because the context of the material discussion is important. Yes, you can change policy at any time, but if it's going to be a major energy policy, whatever the sector it is, it's a commitment if it's won in that direction. We are familiar with this in a discussion about the energy package, e.g. that if there was some kind of debate about energy use and some form of cooperation, and then a change in policy would be possible in one or another if such plans were lost. Depends on how long it had come. So government policy is important in the big context of how much is actually being activated and then it affects priorities.\n",
      "\n",
      "Score: 0.5889 | Utterance: One is that the right person favors above all, and that the right price is paid for the quality, value, and even loss as well. The debate has been loud enough to suggest that the global damage caused by emissions of contaminated gases and chemicals has not been fully evaluated. First, contaminated emissions cause damage to natural ecosystems, which are difficult to assess for money but can have a long impact on the future as we know. Second, climate change brings health harm. Third, climate change causes a global crop failure and a lack that can, in turn, directly affect food prices in this country. Fourth, when the government fails to achieve its objective, the United States makes financial commitments on the basis of international climate agreements. This financial program contains e.g. 800 million. K. There's earmarks on the Kyoto Protocol. That's how long it can be counted. The requirement is that the financial cost of pollution in this way is appreciated and paid accordingly. As long as this is not done, there is a market error that makes it more profitable to pollute rather than to avoid contamination. This is called the pollution prevention and the contaminated cities. In turn, public programs of such bankruptcy can be protected in support of green innovation or a general tax reduction. Thus a double incentive could be created to reduce emissions and help us reach our climate targets. I'd like to ask the highest. Minister if he has entered this discussion and what his position would be in order for the disposal of contaminated gases and chemicals to be reimbursed according to the financial cost of it.\n",
      "\n",
      "Score: 0.5810 | Utterance: I present a committee party on the matter of changing climate laws, No. 70/2012, with respect to simple correction of the part. There's a nominee on the issue. It is very simple and self-explanatory, with which all members of the environment and transport committee are writing.\n",
      "\n",
      "Score: 0.5741 | Utterance: During my final talk, I discussed agriculture and transportation, and I focus on only the climate and the environment. All of this, of course, is closely connected, and obviously you get into the energy situation thereafter. But one thing that this government has been willing to spend a lot of money on without knowing what he's getting into earlier is climate presenting new plans there, no doubt designed to appeal to its supporters a party here in the government and to someone in the international community, which is done without a good explanation of how the funds are being gained. Clearly, the point is that the amount of money that is placed in the lawsuit marked it will be far more beneficial to us than we have seen in the government over time. There is not always a direct connection. It's not that long ago. At the council, the secretary of finance said that it would not just be enough to place more and more money on the health system, so it would not help unless the system was fixed. So, what can be said about the plans and promises of the government in these climates, which introduced plans to put 50 billion on climates without having any real answers to how to use this money so that, of course, the danger is that they go to the air in this sense and become unreasonable, and I go unsatisfactory. It's even mine. and not least that they will not be able to cope with climate, climate change as effectively as we could do here on the land by focusing especially on the strong infrastructure we have in environmental energy production and its expansion and technological development that can result. It is true here in this program that forestry and agriculture will continue to be supported, and therefore I rejoice. The best thing we can do in the environment is to grow the land and especially to grow forests. At the same time, however, more and more money is being spent in action, which actually works against the benefits we can achieve, to form by forestry, and there I speak, for example. of the restoration of wetlands, a special feature of these government spending teams. It allows huge interest in budgets year after year, and despite the development of some kind of fund that has been created to attack the canals and damage farms. It is very unclear what such a result is in climates, but it can even be negative. But like any other government fantasy, it does not interfere with spending money on such matters. Yet, it is obvious that planting trees in a land that has been partially dried brings far more results for climate, carbon binding than for planning to destroy the land and turn it into wetlands. It's all in the same book, Madam President. It is not a matter of making decisions based on content and real influences but on some outward show and bandage. But now I'm just getting through the entrance to my environmental coverage and asking for it to the highest degree. President putting me back on the rosary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "\n",
    "question = \"What is the government policy on climate change?\"\n",
    "# question = \"America?\"\n",
    "k = 5  # choose how many results you want\n",
    "\n",
    "# 1. Embed the question\n",
    "question_embedding = st_model_small.encode(question)\n",
    "\n",
    "# 2. Compute cosine similarities\n",
    "cosine_similarities = util.cos_sim(question_embedding, df_parlamint_embeddings_per_utterance)[0].cpu().numpy()\n",
    "\n",
    "# 3. Get indices of top-k most similar utterances\n",
    "top_k_idx = np.argsort(cosine_similarities)[::-1][:k]\n",
    "\n",
    "# 4. Retrieve the top-k utterances and their similarity scores\n",
    "for idx in top_k_idx:\n",
    "    text = df_parlamint_grouped.iloc[idx][\"utterance_text\"]\n",
    "    score = cosine_similarities[idx]\n",
    "    print(f\"Score: {score:.4f} | Utterance: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef80c25fa4666990",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Adding It All Together\n",
    "\n",
    "### üü¢ The Journey So Far\n",
    "- **Sparse Vectors (BoW, TF-IDF):**\n",
    "  High-dimensional, simple counts, no semantics\n",
    "- **Dense Word Embeddings (Word2Vec, GloVe):**\n",
    "  Compact vectors capturing basic word meaning\n",
    "- **Contextual Models (BERT):**\n",
    "  Token embeddings adapt to context\n",
    "- **Sentence Transformers (SBERT):**\n",
    "  Sentence-level semantic embeddings for similarity & search\n",
    "- **Chunking Strategies:**\n",
    "  Break long texts into meaningful, model-friendly pieces\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Why It Matters\n",
    "- Transform raw text into **meaningful numerical representations**\n",
    "- Enable **semantic search, clustering, Q&A, recommendations**\n",
    "- Foundation for **modern NLP pipelines & AI assistants**\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Key Takeaway\n",
    "A well-designed embedding pipeline =\n",
    "**Chunking + Contextual Models + Smart Similarity Metrics**\n",
    "‚Üí Powerful, scalable text understanding!\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üèãÔ∏è Exercises: Hands-On with Embeddings\n",
    "\n",
    "### üü† Generate embeddings for the Parlamint (sub) dataset\n",
    "- **Experiment with different embedding techniques:**\n",
    "  - Compare different embedding techniques (dense vs sparse) regarding (i) vector dimensionality (ii) Semantic similarity (are similar texts actually closer together?)\n",
    "  - Suggested algorithms: [BERTopic Models](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html) or [HuggingFace (general)](https://huggingface.co/models?other=embeddings&sort=trending) or [Huggingface (sentence transformers)](https://huggingface.co/sentence-transformers/models)\n",
    "- **Consider different chunking techniques:**\n",
    "  - Sentence based vs utterance level vs ??\n",
    "- **Save them as pickle file(s):** `df_dataset.to_pickle(\"<path_and_filename>.pkl\")`\n",
    "\n",
    "### üü† Generate embeddings for the HSA (sub) dataset\n",
    "- **Same tasks as for the Parlamint dataset**\n",
    "\n",
    "### üü† Retrieval: Play around with embeddings and similarity retrieval\n",
    "- **Write queries:**\n",
    "  - Search for valid topics, write queries and manually evaluate the result\n",
    "- **Consider different chunking techniques:**\n",
    "  - Sentence based vs utterance level\n",
    "  - Are topics semantically better captured on sentence level or utterance level=\n",
    "- **Utilize different embedding models for retrieval:**\n",
    "  - Sparse vs dense embeddings\n",
    "  - Experiment with different similarity scores"
   ],
   "id": "69ce67854db5cca3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
