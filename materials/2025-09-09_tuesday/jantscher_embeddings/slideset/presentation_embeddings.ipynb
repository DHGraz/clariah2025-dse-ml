{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6778d16863b961a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<!-- Title Slide -->\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"width:40%; vertical-align:middle; text-align:center;\">\n",
    "  <img src=\"https://maartengr.github.io/BERTopic/logo.png\" alt=\"Topic Modeling Illustration\" width=\"300\"/>\n",
    "</td>\n",
    "<td style=\"width:60%; vertical-align:middle; padding-left:20px;\">\n",
    "\n",
    "  # üìù Topic Modeling with BERTopic\n",
    "  ## Session: Text Embeddings\n",
    "\n",
    "  <br>\n",
    "  <br>\n",
    "  <span style=\"font-size:1.2em; color:gray;\">\n",
    "    Michael Jantscher ¬∑ TU Graz ¬∑ Know Center Research GmbH\n",
    "  </span>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1979fd664ce57d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üëã Michael Jantscher\n",
    "\n",
    "<img src=\"./images/profil_michael.jpg\" alt=\"Michael Jantscher\" width=\"250\"/>\n",
    "\n",
    "**PhD Student** - TU Graz <br>\n",
    "**Researcher** - Know Center Research GmbH\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Focus Areas\n",
    "- Natural Language Processing (NLP) in medical & clinical domains\n",
    "- Causal reasoning in healthcare and (neuro)radiology\n",
    "- Agentic AI systems for decision support and research workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d813d562d2c91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üìå What is a Text Embedding Vector ?- Formal Definition\n",
    "* Numerical representation of text (words, sentences or documents) in a multi-dimensional space\n",
    "* Captures meaning and context\n",
    "* Semantic similar words/sentences/documents -> vectors closer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e96c322fe22eb86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T07:17:55.367947Z",
     "start_time": "2025-09-08T07:17:55.360890Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".reveal .slides section { text-align: left !important; }\n",
       ".reveal h1, .reveal h2, .reveal h3, .reveal p, .reveal table { text-align: left !important; }\n",
       ".reveal table th, .reveal table td { text-align: left !important; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal .slides section { text-align: left !important; }\n",
    ".reveal h1, .reveal h2, .reveal h3, .reveal p, .reveal table { text-align: left !important; }\n",
    ".reveal table th, .reveal table td { text-align: left !important; }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bffbeba45a9191b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:19:39.427869Z",
     "start_time": "2025-09-05T10:19:32.986696Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really like this summer school!</td>\n",
       "      <td>[-0.057146158, -0.053543467, 0.045457065, 0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text  \\\n",
       "0  I really like this summer school!   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.057146158, -0.053543467, 0.045457065, 0.01...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model_small = SentenceTransformer('all-minilm-l6-v2')\n",
    "\n",
    "sample_string = \"I really like this summer school!\"\n",
    "\n",
    "sample_string_embedding = st_model_small.encode(sample_string)\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [sample_string],\n",
    "    \"embedding\": [sample_string_embedding]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01d9f9940574c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Why Use Embedding Vectors Instead of Strings?\n",
    "\n",
    "### üü¢ Drawbacks with Raw Strings\n",
    "- Computers see `\"dog\"` as just `\"d\", \"o\", \"g\"` ‚Äî no understanding of meaning\n",
    "- Hard to measure **similarity** or **relationships** between words\n",
    "- Not suitable for **search, clustering, or ML models**\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Benefits of Embeddings\n",
    "- **Numerical Representation:** Converts text into vectors that algorithms understand\n",
    "- **Capture Meaning:** Similar words or sentences are close in vector space\n",
    "- **Efficient & Scalable:** Enables fast similarity search with dot product or cosine similarity\n",
    "- **Handle Synonyms & Context:** `\"car\"` ‚âà `\"automobile\"`, context-aware models disambiguate `\"bank\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Example\n",
    "\n",
    "| Text      | Raw Representation         | Embedding (Example)          |\n",
    "|-----------|---------------------------|-----------------------------|\n",
    "| `\"dog\"`   | `\"d\", \"o\", \"g\"`           | `[0.12, -0.45, 0.87, ...]`  |\n",
    "| `\"puppy\"` | `\"p\", \"u\", \"p\", \"p\", \"y\"` | `[0.11, -0.48, 0.90, ...]`  |\n",
    "\n",
    "*Vectors are close ‚Üí words are semantically similar*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50878fcfb64d0a99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üí° Uses Cases of Text Embedding Vectors\n",
    "\n",
    "| **Use Case**                   | **Example** |\n",
    "|--------------------------------|-------------|\n",
    "| **Semantic Search**         | Search ‚Äúdoctor‚Äù ‚Üí find content on ‚Äúphysicians‚Äù or ‚Äúhealthcare providers‚Äù even without exact keywords |\n",
    "| **Recommendation Systems**  | Suggest similar research papers, movies, or products based on descriptions or reviews |\n",
    "| **Sentiment Analysis**      | Understand tone (positive/negative/neutral) beyond simple keywords in tweets or reviews |\n",
    "| **Clustering & Topic Modeling** | Group thousands of news articles or support tickets by topic automatically |\n",
    "| **Chatbots & Virtual Assistants** | Improve NLU so bots answer contextually, not just by keyword |\n",
    "| **Fraud Detection**        | Spot unusual or suspicious text patterns in financial or insurance claims |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4ade64327479a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# ü¶ñ Pre-Embedding Area\n",
    "üìå **Definition:**\n",
    "Early NLP approaches represented text as **sparse, high-dimensional vectors**.\n",
    "Each dimension corresponded to a **unique word or token**, with no sense of meaning or context.\n",
    "\n",
    "---\n",
    "\n",
    "### Bag-of-Words (BoW)\n",
    "- Represents documents as a **vector of word counts**\n",
    "- Ignores grammar, order, and semantics\n",
    "- Example:\n",
    "  `\"I like NLP\"` ‚Üí `[1, 1, 1, 0, 0, ...]`\n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)\n",
    "- Adjusts raw counts to emphasize **rare, informative words** and downweight common words\n",
    "- Example: ‚Äúthe‚Äù ‚Üí low weight, ‚Äúquantum‚Äù ‚Üí high weight\n",
    "\n",
    "---\n",
    "\n",
    "#### TF-IDF Formula\n",
    "\n",
    "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times \\log\\left(\\frac{N}{DF(t)}\\right)$$\n",
    "\n",
    "Where:\n",
    "- (TF(t, d)\\): Frequency of term \\(t\\) in document \\(d\\)\n",
    "- \\(DF(t)\\): Number of documents containing \\(t\\)\n",
    "- \\(N\\): Total number of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d140f851dea732e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:19:28.376173Z",
     "start_time": "2025-09-06T18:19:27.832452Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load parlamint dataset\n",
    "df_parlamint = pd.read_csv(\"../../../datasets/parlamint/parlamint-it-is-2022.txt\", sep=\"\\t\").head(2000)\n",
    "df_parlamint_subset = df_parlamint.head(1000).copy(deep=True)\n",
    "df_parlamint\n",
    "\n",
    "# Group sentence by utterance (=Parent_ID)\n",
    "df_parlamint_grouped = (df_parlamint.groupby([\"Parent_ID\"])[\"Text\"]\n",
    "                        .apply(lambda s: \" \".join(s))\n",
    "                        .reset_index(name=\"utterance_text\")).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33eb4aeaadc7d2dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:19:30.672610Z",
     "start_time": "2025-09-06T18:19:30.666139Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Parent_ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg2.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>President of the United States reports:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg3.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>I have decided, according to the proposal of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg4.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Arrange sites, January 11th, 2022.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg6.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Katr√≠n Jakobsd√≥ttir's daughter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg7.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Presidential Letters for a meeting of the Gene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID                      Parent_ID  \\\n",
       "0  ParlaMint-IS_2022-01-17-20.seg2.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "1  ParlaMint-IS_2022-01-17-20.seg3.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "2  ParlaMint-IS_2022-01-17-20.seg4.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "3  ParlaMint-IS_2022-01-17-20.seg6.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "4  ParlaMint-IS_2022-01-17-20.seg7.1  ParlaMint-IS_2022-01-17-20.u1   \n",
       "\n",
       "                                                Text  \n",
       "0            President of the United States reports:  \n",
       "1  I have decided, according to the proposal of t...  \n",
       "2                 Arrange sites, January 11th, 2022.  \n",
       "3                    Katr√≠n Jakobsd√≥ttir's daughter.  \n",
       "4  Presidential Letters for a meeting of the Gene...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parlamint.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43f42ed766325e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:19:32.883089Z",
     "start_time": "2025-09-06T18:19:32.873780Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parent_ID</th>\n",
       "      <th>utterance_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>President of the United States reports: I have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u10</td>\n",
       "      <td>Before the weekend, an article by Stef√°nssonar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u11</td>\n",
       "      <td>I read this decision in Perconte, which is not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u12</td>\n",
       "      <td>In fact, this is shown in the letter quoted by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u13</td>\n",
       "      <td>Yes, that's right. That's right. A senator who...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Parent_ID  \\\n",
       "0   ParlaMint-IS_2022-01-17-20.u1   \n",
       "1  ParlaMint-IS_2022-01-17-20.u10   \n",
       "2  ParlaMint-IS_2022-01-17-20.u11   \n",
       "3  ParlaMint-IS_2022-01-17-20.u12   \n",
       "4  ParlaMint-IS_2022-01-17-20.u13   \n",
       "\n",
       "                                      utterance_text  \n",
       "0  President of the United States reports: I have...  \n",
       "1  Before the weekend, an article by Stef√°nssonar...  \n",
       "2  I read this decision in Perconte, which is not...  \n",
       "3  In fact, this is shown in the letter quoted by...  \n",
       "4  Yes, that's right. That's right. A senator who...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parlamint_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340e635d8fcb036b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:19:34.721594Z",
     "start_time": "2025-09-06T18:19:34.697232Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President of the United States reports:\n",
      "I have decided, according to the proposal of the prime minister, that the Council should meet for an extended meeting on Monday, January 17, 2022 p.m. 3:00.\n",
      "Arrange sites, January 11th, 2022.\n",
      "Katr√≠n Jakobsd√≥ttir's daughter.\n",
      "Presidential Letters for a meeting of the General Assembly for a subsequent meeting on January 17, 2022\n",
      "I'd like to use this opportunity here after reading this letter and offer the highest. President and w. Senators welcome to New Year's Parliamentary Conferences.\n"
     ]
    }
   ],
   "source": [
    "# Take a single utterance from the dataset\n",
    "sample_utterance = df_parlamint[df_parlamint[\"Parent_ID\"] == \"ParlaMint-IS_2022-01-17-20.u1\"][\"Text\"]\n",
    "print(\"\\n\".join([e for e in sample_utterance]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9385afaac9900eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:19:37.021775Z",
     "start_time": "2025-09-06T18:19:37.008613Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 'transform' only...\n",
      "Number features: 5 ['17 2022' '2022' 'january' 'january 17' 'meeting']\n",
      "Shape embedding array: (6, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17 2022</th>\n",
       "      <th>2022</th>\n",
       "      <th>january</th>\n",
       "      <th>january 17</th>\n",
       "      <th>meeting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   17 2022  2022  january  january 17  meeting\n",
       "0        0     0        0           0        0\n",
       "1        1     1        1           1        1\n",
       "2        0     1        1           0        0\n",
       "3        0     0        0           0        0\n",
       "4        1     1        1           1        2\n",
       "5        0     0        0           0        0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import CountVectorizerEmbedder\n",
    "\n",
    "# Adding just the utterance sample as vocabulary\n",
    "cv_model = CountVectorizerEmbedder(vocabulary=sample_utterance, max_features=5, stop_words='english',\n",
    "                                   ngram_range=(1, 2))\n",
    "#cv_model = CountVectorizerEmbedder(vocabulary=sample_utterance)\n",
    "\n",
    "cv_embeddings = cv_model.embed(sample_utterance)\n",
    "print(f\"Number features: {len(cv_model.embedding_model.get_feature_names_out())}\", cv_model.embedding_model.get_feature_names_out())\n",
    "print(f\"Shape embedding array: {cv_embeddings.toarray().shape}\")\n",
    "df_cv_output = pd.DataFrame(columns=cv_model.embedding_model.get_feature_names_out(), data=cv_embeddings.toarray())\n",
    "df_cv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b168738213bc1973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:19:41.407425Z",
     "start_time": "2025-09-06T18:19:41.393317Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 'transform' only...\n",
      "Number features: 5 ['17' '2022' 'january' 'meeting' 'president']\n",
      "Shape embedding array: (6, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17</th>\n",
       "      <th>2022</th>\n",
       "      <th>january</th>\n",
       "      <th>meeting</th>\n",
       "      <th>president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.540298</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.540298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.394497</td>\n",
       "      <td>0.333062</td>\n",
       "      <td>0.333062</td>\n",
       "      <td>0.788994</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         17      2022   january   meeting  president\n",
       "0  0.000000  0.000000  0.000000  0.000000        1.0\n",
       "1  0.540298  0.456156  0.456156  0.540298        0.0\n",
       "2  0.000000  0.707107  0.707107  0.000000        0.0\n",
       "3  0.000000  0.000000  0.000000  0.000000        0.0\n",
       "4  0.394497  0.333062  0.333062  0.788994        0.0\n",
       "5  0.000000  0.000000  0.000000  0.000000        1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import TfIdfEmbedder\n",
    "\n",
    "# Adding just the utterance sample as vocabulary\n",
    "tfidf_model = TfIdfEmbedder(vocabulary=sample_utterance, max_features=5, stop_words='english')\n",
    "tfidf_embeddings = tfidf_model.embed(sample_utterance)\n",
    "print(f\"Number features: {len(tfidf_model.embedding_model.get_feature_names_out())}\", tfidf_model.embedding_model.get_feature_names_out())\n",
    "print(f\"Shape embedding array: {tfidf_embeddings.toarray().shape}\")\n",
    "df_tfidf_output = pd.DataFrame(columns=tfidf_model.embedding_model.get_feature_names_out(), data=tfidf_embeddings.toarray())\n",
    "df_tfidf_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0f058c66c231d",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dense Text Embeddings\n",
    "\n",
    "üìå **Definition:**\n",
    "Dense embeddings represent words, sentences, or documents as **low-dimensional, dense vectors**\n",
    "where similar meanings are **close together in vector space**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Characteristics\n",
    "- **Low-dimensional** (e.g., 100‚Äì1,536 dimensions, not vocab-sized)\n",
    "- **Dense representation**: most values ‚â† 0\n",
    "- Captures **semantic meaning** and context\n",
    "- Learned from data via **neural networks**\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Brief History\n",
    "- **Word2Vec (2013)** ‚Äì First widely used dense word embeddings (Mikolov et al.)\n",
    "- **GloVe (2014)** ‚Äì Global Vectors for word representation\n",
    "- **FastText (2016)** ‚Äì Adds subword information for better handling of rare words\n",
    "- **ELMo (2018)** ‚Äì Contextual word embeddings\n",
    "- **BERT (2018)** ‚Äì Contextual embeddings for entire sentences\n",
    "- **OpenAI / Modern Embeddings (2020s)** ‚Äì High-quality sentence/document embeddings (e.g., `text-embedding-3-large`)\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Why It‚Äôs Better\n",
    "- Reduces dimensionality dramatically\n",
    "- Learns **semantic relationships**\n",
    "- Powers modern **search, recommendation, and AI assistants**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9a28e28a13c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T10:59:22.623450Z",
     "start_time": "2025-09-05T10:59:22.617960Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word2Vec: CBOW & Skip-Gram Overview\n",
    "\n",
    "### üü¢ What is Word2Vec?\n",
    "- A **shallow, two-layer neural network** that learns word embeddings from context.\n",
    "- Maps words to **dense vectors** in a continuous space; similar words are **close together**.\n",
    "- Introduced by Mikolov et al., **2013**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Architectures\n",
    "\n",
    "#### üîπ Continuous Bag-of-Words (CBOW)\n",
    "- Predicts the **center word** given its context words.\n",
    "- Fast to train; works well for **frequent words**.\n",
    "\n",
    "#### üîπ Skip-Gram\n",
    "- Predicts **context words** given a center word.\n",
    "- Performs better for **rare words**, large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Why It Matters\n",
    "- Captures **semantic relationships**:\n",
    "  `king - man + woman ‚âà queen`\n",
    "- Major leap from sparse (BoW/TF-IDF) to **dense, meaningful embeddings**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532683a1f125d49d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word2Vec: Training & Visual Intuition\n",
    "\n",
    "### üü¢ Training Workflow\n",
    "1. **One-hot encoding** for words.\n",
    "2. **Hidden layer** = embedding lookup table.\n",
    "3. **Output layer** predicts context words (softmax with negative sampling).\n",
    "4. Final **hidden layer weights** = embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Visual Intuition\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "<img src=\"./images/word2vec_diagrams.png\" alt=\"Word2Vec Architecture\" style=\"width:100%;\">\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "  <img src=\"./images/skip_gram_net_arch.png\" alt=\"Word2Vec Context\" style=\"width:100%;\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- Left: Skip-Gram architecture predicting context words.\n",
    "- Right: Example of context window for a target word.\n",
    "- Only the **embedding layer weights** are retained.\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Israel G. (2017). *Word2Vec Explained*.\n",
    "  [https://israelg99.github.io/2017-03-23-Word2Vec-Explained/](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1a24c6a4ca3de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT: Contextual Embeddings\n",
    "\n",
    "### üü¢ What is BERT?\n",
    "- **Bidirectional Encoder Representations from Transformers** (2018, Google AI).\n",
    "- Uses **Transformer architecture** to create **contextual word embeddings**:\n",
    "  - Each word‚Äôs vector depends on **all surrounding words** (left & right context).\n",
    "- Trained on **masked language modeling** and **next sentence prediction** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Key Innovations\n",
    "- **Bidirectional**: Unlike Word2Vec/Glove, captures context from both sides.\n",
    "- **Transformer encoder layers** with self-attention results in rich, deep embeddings.\n",
    "- **Contextualization**: Same word gets **different vectors** depending on context\n",
    "  (‚Äúbank‚Äù in ‚Äúriver bank‚Äù vs. ‚Äúbank account‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Visual Intuition\n",
    "\n",
    "<img src=\"images/bert_embeddings.png\" alt=\"BERT Transformer\" style=\"width:40%;\">\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Devlin et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.*\n",
    "  [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80320a1691db2328",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Sentence Transformers Recap & Training\n",
    "\n",
    "### üü¢ Key Takeaways\n",
    "- **Sparse vectors** (BoW, TF-IDF):\n",
    "  - One dimension per word, mostly zeros\n",
    "  - No deep semantic meaning\n",
    "- **Dense embeddings** (Word2Vec, BERT, SBERT):\n",
    "  - Low-dimensional, rich semantic context\n",
    "  - Words/sentences cluster by meaning\n",
    "\n",
    "---\n",
    "\n",
    "### üü† How SBERT is Trained\n",
    "- **Backbone:** Pretrained BERT or RoBERTa encoders\n",
    "- **Siamese/Triplet Network Architecture:**\n",
    "  - Encodes two or three sentences **independently** into embeddings\n",
    "  - Trains to minimize distance for similar sentences and maximize for dissimilar ones\n",
    "- **Training Objectives [Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html):**\n",
    "  - **Contrastive loss** (distance-based similarity)\n",
    "  - **Natural Language Inference** datasets (entailment, contradiction, neutral)\n",
    "  - **MultipleNegativesRankingLoss** for retrieval tasks\n",
    "- **Result:**\n",
    "  - Embeddings suitable for **cosine similarity** ‚Üí semantic search, clustering, recommendations\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Reimers & Gurevych (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*\n",
    "  [https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)\n",
    "- SentenceTransformers Documentation [https://www.sbert.net/](https://www.sbert.net/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c076afcd42b56e09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:19:51.780476Z",
     "start_time": "2025-09-06T18:19:48.236554Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really like this summer school!</td>\n",
       "      <td>[-0.057146158, -0.053543467, 0.045457065, 0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text  \\\n",
       "0  I really like this summer school!   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.057146158, -0.053543467, 0.045457065, 0.01...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model_small = SentenceTransformer('all-minilm-l6-v2')\n",
    "\n",
    "sample_string = \"I really like this summer school!\"\n",
    "\n",
    "sample_string_embedding = st_model_small.encode(sample_string)\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [sample_string],\n",
    "    \"embedding\": [sample_string_embedding]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ce1437f2cd776",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Distance & Similarity Measures (Brief Intro)\n",
    "\n",
    "### üü¢ Why It Matters\n",
    "- Compare embeddings ‚Üí find **semantic similarity** between words, sentences, or documents.\n",
    "- Core to **semantic search, clustering, and topic modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Common Measures\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "üîπ <b>Dot Product</b>\n",
    "Unnormalized similarity; sensitive to magnitude:\n",
    "$$\n",
    "a \\cdot b = \\sum a_i b_i\n",
    "$$\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<img src=\"images/dot_prod.png\" alt=\"Dot Product\" width=\"200\" align=\"right\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "üîπ <b>Cosine Similarity</b>\n",
    "  Measures <b>angle</b> between vectors (ignores magnitude):\n",
    "  $$\n",
    "  \\text{cosine\\_sim}(a, b) = \\frac{a \\cdot b}{\\|a\\|\\|b\\|}\n",
    "  $$\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<img src=\"images/dot_prod.png\" alt=\"Dot Product\" width=\"200\" align=\"right\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28deac15c504e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Distance & Similarity Measures (Brief Intro)\n",
    "\n",
    "### üü† Common Measures cont'd\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "üîπ <b>Euclidean Distance</b>\n",
    "  Straight-line distance in vector space:\n",
    "  $$\n",
    "  d(a, b) = \\sqrt{\\sum (a_i - b_i)^2}\n",
    "  $$\n",
    " </td>\n",
    "<td width=\"30%\">\n",
    "<img src=\"images/euc_distance.png\" alt=\"Dot Product\" width=\"200\" align=\"right\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "üîπ <b>Manhattan (L1) Distance</b>\n",
    "  Sum of absolute differences:\n",
    "  $$\n",
    "  d_{\\text{L1}}(a, b) = \\sum |a_i - b_i|\n",
    "  $$\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<img src=\"images/manhattan_distance.png\" alt=\"Dot Product\" width=\"200\" align=\"right\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060a9ae18f92011",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Chunking Techniques for Embeddings\n",
    "\n",
    "### üü¢ Why Chunking?\n",
    "- Long documents exceed model token limits (e.g., BERT ~512 tokens).\n",
    "- Splitting text into **manageable chunks** improves:\n",
    "  - ‚úÖ Embedding quality\n",
    "  - ‚úÖ Retrieval accuracy\n",
    "  - ‚úÖ Context management in downstream tasks\n",
    "- üéØ **Goal of Good Chunking:**\n",
    "  Create chunks that are **small enough** to fit model limits\n",
    "  but **large enough** to retain full semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Common Techniques\n",
    "\n",
    "1. **Fixed-Length Chunking**\n",
    "   - Split text into chunks of `N` tokens/words.\n",
    "   - Simple, fast, but can cut off sentences mid-way.\n",
    "\n",
    "2. **Sentence-Based Chunking**\n",
    "   - Split by sentence boundaries (NLTK, spaCy).\n",
    "   - Better for readability, semantic grouping.\n",
    "\n",
    "3. **Paragraph-Based Chunking**\n",
    "   - Keep natural paragraph structure.\n",
    "   - Good for preserving context, but chunk sizes vary.\n",
    "\n",
    "4. **Sliding Window / Overlapping Chunks**\n",
    "   - Add overlap between chunks (e.g., 50 tokens).\n",
    "   - Prevents loss of context between splits.\n",
    "\n",
    "5. **Semantic Chunking**\n",
    "   - Use topic segmentation or embeddings to find boundaries.\n",
    "   - Most accurate, but computationally heavier.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Choose technique based on document length, model limits, and retrieval needs.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860bdd8b5a60922",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üèãÔ∏è Exercise: Simple Embedding Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c99b05779465c099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:20:51.920704Z",
     "start_time": "2025-09-06T18:20:06.836493Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 37.61it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00<00:00, 160.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode utterance-wise dataset\n",
    "df_parlamint_embeddings_per_utterance = st_model_small.encode(df_parlamint_grouped[\"utterance_text\"].to_list(),\n",
    "                                                     show_progress_bar=True)\n",
    "\n",
    "# Encode sentence-wise dataset\n",
    "df_parlamint_embeddings_per_sentence = st_model_small.encode(df_parlamint[\"Text\"].to_list(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fa2bd617d780a64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:20:55.226150Z",
     "start_time": "2025-09-06T18:20:55.167525Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Parent_ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg2.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>President of the United States reports:</td>\n",
       "      <td>[0.0036431556, 0.0075752744, -0.013524163, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg3.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>I have decided, according to the proposal of t...</td>\n",
       "      <td>[-0.047604736, -0.06898866, 0.027104922, 0.042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg4.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Arrange sites, January 11th, 2022.</td>\n",
       "      <td>[-0.016980143, -0.04804505, -0.01744971, 0.010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg6.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Katr√≠n Jakobsd√≥ttir's daughter.</td>\n",
       "      <td>[-0.08331401, -0.04858722, 0.00143908, -0.0432...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.seg7.1</td>\n",
       "      <td>ParlaMint-IS_2022-01-17-20.u1</td>\n",
       "      <td>Presidential Letters for a meeting of the Gene...</td>\n",
       "      <td>[-0.07626823, -0.06527784, 0.06610001, 0.01071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.seg265.3</td>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.u77</td>\n",
       "      <td>It's actually a very interesting case. A senat...</td>\n",
       "      <td>[-0.0054431674, 0.033196796, 0.07330073, 0.021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.seg265.4</td>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.u77</td>\n",
       "      <td>Maybe it's just a matter of deep discussion in...</td>\n",
       "      <td>[0.008967635, -0.099709705, 0.020336002, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.seg265.5</td>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.u77</td>\n",
       "      <td>The fact is, this is not some new word.</td>\n",
       "      <td>[0.031318653, -0.08153122, -0.036312055, 0.024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.seg265.6</td>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.u77</td>\n",
       "      <td>It is not taking the planning of rural society...</td>\n",
       "      <td>[0.08973158, -0.009728343, 0.032514274, 0.0379...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.seg265.7</td>\n",
       "      <td>ParlaMint-IS_2022-01-18-23.u77</td>\n",
       "      <td>This is primarily true of public airport plann...</td>\n",
       "      <td>[0.14793488, -0.049361013, 0.041799046, 0.0372...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ID                       Parent_ID  \\\n",
       "0       ParlaMint-IS_2022-01-17-20.seg2.1   ParlaMint-IS_2022-01-17-20.u1   \n",
       "1       ParlaMint-IS_2022-01-17-20.seg3.1   ParlaMint-IS_2022-01-17-20.u1   \n",
       "2       ParlaMint-IS_2022-01-17-20.seg4.1   ParlaMint-IS_2022-01-17-20.u1   \n",
       "3       ParlaMint-IS_2022-01-17-20.seg6.1   ParlaMint-IS_2022-01-17-20.u1   \n",
       "4       ParlaMint-IS_2022-01-17-20.seg7.1   ParlaMint-IS_2022-01-17-20.u1   \n",
       "...                                   ...                             ...   \n",
       "1995  ParlaMint-IS_2022-01-18-23.seg265.3  ParlaMint-IS_2022-01-18-23.u77   \n",
       "1996  ParlaMint-IS_2022-01-18-23.seg265.4  ParlaMint-IS_2022-01-18-23.u77   \n",
       "1997  ParlaMint-IS_2022-01-18-23.seg265.5  ParlaMint-IS_2022-01-18-23.u77   \n",
       "1998  ParlaMint-IS_2022-01-18-23.seg265.6  ParlaMint-IS_2022-01-18-23.u77   \n",
       "1999  ParlaMint-IS_2022-01-18-23.seg265.7  ParlaMint-IS_2022-01-18-23.u77   \n",
       "\n",
       "                                                   Text  \\\n",
       "0               President of the United States reports:   \n",
       "1     I have decided, according to the proposal of t...   \n",
       "2                    Arrange sites, January 11th, 2022.   \n",
       "3                       Katr√≠n Jakobsd√≥ttir's daughter.   \n",
       "4     Presidential Letters for a meeting of the Gene...   \n",
       "...                                                 ...   \n",
       "1995  It's actually a very interesting case. A senat...   \n",
       "1996  Maybe it's just a matter of deep discussion in...   \n",
       "1997            The fact is, this is not some new word.   \n",
       "1998  It is not taking the planning of rural society...   \n",
       "1999  This is primarily true of public airport plann...   \n",
       "\n",
       "                                              embedding  \n",
       "0     [0.0036431556, 0.0075752744, -0.013524163, 0.0...  \n",
       "1     [-0.047604736, -0.06898866, 0.027104922, 0.042...  \n",
       "2     [-0.016980143, -0.04804505, -0.01744971, 0.010...  \n",
       "3     [-0.08331401, -0.04858722, 0.00143908, -0.0432...  \n",
       "4     [-0.07626823, -0.06527784, 0.06610001, 0.01071...  \n",
       "...                                                 ...  \n",
       "1995  [-0.0054431674, 0.033196796, 0.07330073, 0.021...  \n",
       "1996  [0.008967635, -0.099709705, 0.020336002, -0.02...  \n",
       "1997  [0.031318653, -0.08153122, -0.036312055, 0.024...  \n",
       "1998  [0.08973158, -0.009728343, 0.032514274, 0.0379...  \n",
       "1999  [0.14793488, -0.049361013, 0.041799046, 0.0372...  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parlamint_grouped[\"embedding\"] = list(df_parlamint_embeddings_per_utterance)\n",
    "df_parlamint[\"embedding\"] = list(df_parlamint_embeddings_per_sentence)\n",
    "df_parlamint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f24c2bd73106548e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T18:20:57.226748Z",
     "start_time": "2025-09-06T18:20:57.201003Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4181 | Utterance: The Organization completed the last year's assessment of the environmental effects of the silica Union Bank in Helga, and last week it was informed of the signing will of the factory's possible sale. The main conclusions of this assessment are, as the report says, by permission of the president: Since induction will be successful, the Organization considers the role of the 1st class of air quality management to be effective, i.e. as Hyperbola ovens, quite a negative ‚óØ[... ] However, the effect of the final production on air quality to four ovens in the same way as planned could be considerably negative. When the report shows that there is something else in my mind that is in harmony with today's ill-eyed views on climate. For example, in the factory 155,000 tons of coal per year will be burned for full-time business. This would be one of Iceland's largest industrial pollutants. The production also results in carbon dioxide emissions from burning on fossil fuels of up to 100,000 tons per year compared to a single arc reactor and up to 400,000 tons a year relative to four ovens. There are also such substances as mercury and arsenic, which are produced in this factory. In addition, many of these substances have a common role in causing respiratory irritation and subsequent respiratory distress, especially in those who are already ill. That's why I want to ask the highest. Prime Minister on her view of this assessment of environmental impact and the declaration of will. Does Ministers believe that a silicar's restarting can harmonize with the outlook and policy of the nation on climate? Is it consistent with the left-green climate policy? Is it compatible with the interests of the nation?\n",
      "\n",
      "Score: 0.3869 | Utterance: Once more often, I come here to discuss careful legislation and careful management here at the General Assembly. The contract and processing of the Parliament needs to consult with the major economists and scholars so that basic viewpoints can be obtained, and it is of no less importance that we here at the council are given sufficient time to avoid making mistakes, sometimes irreversible. Sadly, at the beginning of the term, we are immediately seeing the opposite in the practices of the government. During the last term, there were cases of confusion, either knowingly or not, that were handled by law. This morning it was reported that the EU has confirmed the violation of environmental assessment legislation in the United States, while the matter is being taken care of by the Parliament on a fish-gain bill taken very quickly on one day in 2018. This is a very serious matter and shows the government's view of environmental decision-making. At that time, the environmental minister of Ingi In Gu√∞brandsson claimed that there was nothing to worry about, and that was the case. Today, the United States government and, of course, the Icelandic public find themselves in violation of the law. The same was true regarding the National Court case and the order of judges who served as ministers here, one by one, claimed to have been within the law, and all in accordance with the rules of law, but all the courts, from abroad, came to the opposite conclusion that there had been a clear violation of the law. It is not a sign of good government here on a terrible project. Even worse is pressured by the Council of Law, which has irreversible consequences and often results in considerable cost to the European public. Let us be more careful, for that is what happened to us.\n",
      "\n",
      "Score: 0.3865 | Utterance: I'm so happy to hear it. A senator is taking the initiative here to put these matters on the program because I believe that it is important that in general about domestic agriculture that we put gardening and gardening into even more foreground than we have ever done. There are especially great conventions in this area that are consistent with both our aims for food security but also the emissions of greenhouse gases and many of the more and more of the 21st century activities and projects that are always ahead of us when faced with the challenges of the 21st century. I look forward to dealing with this aspect of my Ministry and just as much as cooperating with it. That's what a senator's got to do with it.\n",
      "\n",
      "Score: 0.3762 | Utterance: Thank you. A senator's inquiry. As noted in the inquiry, the staff was appointed by tourism, industrial and innovation, and the work was carried out by that part of the Department of Employment and Development. However, there is the value of farmers' payments on electricity, which I agree with with. A senator being a very important part of the work conditions of this important agricultural growth mitigation, it is clear that the provision of the manufacturer's products depends on the domestic agreements, i.e. agreement on the working conditions of the manufacturers of the horticultural products. A contract of change was signed on May 14, 2020, but the agreement was first added to the garden contribution, 200 million per year, starting in 2020, to the duration of the contract, of which 70 million were added each year to pay especially for electricity. So, according to the contract, there's about 385 million. K., updated every year to pay for electrical energy and climatic sources were guaranteed outright due to lighting instead of subsidies, as per this agreement. Funds pay to the manufacturer according to the use of electricity for greenhouse descriptions. By references, these manufacturers are actually committed direct payments in order to help improve the work environment of the article. But on the ground, I agree with what. A senator that we need to do better than that. We need to set a high goal, in view of the better conditions for the production of vegetables in Iceland. They are the vast number of parishes available in that part of this article.\n",
      "\n",
      "Score: 0.3620 | Utterance: We have been doing a little since the outbreak began calling because the council has more access to the measures being taken and has a role in the process of reviewing these issues. We were asked at the time for a health minister to be given here every two weeks a general report on the disease control status. But we've taken it a little further by sending the President of the Generals a letter asking us to have a health minister and financial minister, and after the events of other ministers, to report the same day or as soon as possible following the introduction of certain disease prevention procedures or extended by the government. Then we request that reporting include information on the basis of decisions, the expected results and impact on society in other ways and that it will be informed on the same day or as soon as possible what measures will be taken to deal with the economic and social consequences of decision-making procedures at one time. It's not just a group of reveries that's been called after this in the council in the past few months, there's been more members of the council. It was noteworthy to hear an excellent speech. paritaprevir Vilhj√°lmur's √Årnason of the Year before, since he very correctly pointed out that the government has gone a little crazy when it comes to granting the council the opportunity to support the Commission. Then I'd like to respond to a question raised by a seizure. paritaprevir Dilj√° Mist Einarsd√≥ttir. I don't like Europe being transformed into one medium-sized and subjugated state, but I'm impressed by the fact that Europe has a strong cooperation in which civil rights, human rights, social justice, and economic progress are put in place rather than by the isolation and capitalism that are often referred to in the halls of the Generalis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "\n",
    "question = \"What is the government policy on climate change?\"\n",
    "# question = \"America?\"\n",
    "k = 5  # choose how many results you want\n",
    "\n",
    "# 1. Embed the question\n",
    "question_embedding = st_model_small.encode(question)\n",
    "\n",
    "# 2. Compute cosine similarities\n",
    "cosine_similarities = util.cos_sim(question_embedding, df_parlamint_embeddings_per_utterance)[0].cpu().numpy()\n",
    "\n",
    "# 3. Get indices of top-k most similar utterances\n",
    "top_k_idx = np.argsort(cosine_similarities)[::-1][:k]\n",
    "\n",
    "# 4. Retrieve the top-k utterances and their similarity scores\n",
    "for idx in top_k_idx:\n",
    "    text = df_parlamint_grouped.iloc[idx][\"utterance_text\"]\n",
    "    score = cosine_similarities[idx]\n",
    "    print(f\"Score: {score:.4f} | Utterance: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef80c25fa4666990",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Adding It All Together\n",
    "\n",
    "### üü¢ The Journey So Far\n",
    "- **Sparse Vectors (BoW, TF-IDF):**\n",
    "  High-dimensional, simple counts, no semantics\n",
    "- **Dense Word Embeddings (Word2Vec, GloVe):**\n",
    "  Compact vectors capturing basic word meaning\n",
    "- **Contextual Models (BERT):**\n",
    "  Token embeddings adapt to context\n",
    "- **Sentence Transformers (SBERT):**\n",
    "  Sentence-level semantic embeddings for similarity & search\n",
    "- **Chunking Strategies:**\n",
    "  Break long texts into meaningful, model-friendly pieces\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Why It Matters\n",
    "- Transform raw text into **meaningful numerical representations**\n",
    "- Enable **semantic search, clustering, Q&A, recommendations**\n",
    "- Foundation for **modern NLP pipelines & AI assistants**\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Key Takeaway\n",
    "A well-designed embedding pipeline =\n",
    "**Chunking + Contextual Models + Smart Similarity Metrics**\n",
    "‚Üí Powerful, scalable text understanding!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ce67854db5cca3",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üèãÔ∏è Exercises: Hands-On with Embeddings\n",
    "\n",
    "### üü† Generate embeddings for the Parlamint (sub) dataset\n",
    "- **Experiment with different embedding techniques:**\n",
    "  - Compare different embedding techniques (dense vs sparse) regarding (i) vector dimensionality (ii) Semantic similarity (are similar texts actually closer together?)\n",
    "  - Suggested algorithms: [BERTopic Models](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html) or [HuggingFace (general)](https://huggingface.co/models?other=embeddings&sort=trending) or [Huggingface (sentence transformers)](https://huggingface.co/sentence-transformers/models)\n",
    "- **Consider different chunking techniques:**\n",
    "  - Sentence based vs utterance level vs ??\n",
    "- **Save them as pickle file(s):** `df_dataset.to_pickle(\"<path_and_filename>.pkl\")`\n",
    "\n",
    "### üü† Generate embeddings for the HSA (sub) dataset\n",
    "- **Same tasks as for the Parlamint dataset**\n",
    "\n",
    "### üü† Retrieval: Play around with embeddings and similarity retrieval\n",
    "- **Write queries:**\n",
    "  - Search for valid topics, write queries and manually evaluate the result\n",
    "- **Consider different chunking techniques:**\n",
    "  - Sentence based vs utterance level\n",
    "  - Are topics semantically better captured on sentence level or utterance level=\n",
    "- **Utilize different embedding models for retrieval:**\n",
    "  - Sparse vs dense embeddings\n",
    "  - Experiment with different similarity scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
