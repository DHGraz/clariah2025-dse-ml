{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<!-- Title Slide -->\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"width:40%; vertical-align:middle; text-align:center;\">\n",
    "  <img src=\"https://maartengr.github.io/BERTopic/logo.png\" alt=\"Topic Modeling Illustration\" width=\"300\"/>\n",
    "</td>\n",
    "<td style=\"width:60%; vertical-align:middle; padding-left:20px;\">\n",
    "\n",
    "  # üìù Topic Modeling with BERTopic\n",
    "  ## Session: Text Embeddings\n",
    "\n",
    "  <br>\n",
    "  <br>\n",
    "  <span style=\"font-size:1.2em; color:gray;\">\n",
    "    Michael Jantscher ¬∑ TU Graz ¬∑ Know Center Research GmbH\n",
    "  </span>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ],
   "id": "27aa5a34063a2f73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üëã Michael Jantscher\n",
    "\n",
    "<img src=\"./images/profil_michael.jpg\" alt=\"Michael Jantscher\" width=\"250\"/>\n",
    "\n",
    "**PhD Student** - TU Graz <br>\n",
    "**Researcher** - Know Center Research GmbH\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Focus Areas\n",
    "- Natural Language Processing (NLP) in medical & clinical domains\n",
    "- Causal reasoning in healthcare and (neuro)radiology\n",
    "- Agentic AI systems for decision support and research workflows\n"
   ],
   "id": "f3c8965e5e8e9dc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìå What is a Text Embedding Vector ?- Formal Definition\n",
    "* Numerical representation of text (words, sentences or documents) in a multi-dimensional space\n",
    "* Captures meaning and context\n",
    "* Semantic similar words/sentences/documents -> vectors closer together"
   ],
   "id": "e583aeef568917b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal .slides section { text-align: left !important; }\n",
    ".reveal h1, .reveal h2, .reveal h3, .reveal p, .reveal table { text-align: left !important; }\n",
    ".reveal table th, .reveal table td { text-align: left !important; }\n",
    "</style>\n",
    "\"\"\")"
   ],
   "id": "5e14a63183c21f17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model_small = SentenceTransformer('all-minilm-l6-v2')\n",
    "\n",
    "sample_string = \"I really like this summer school!\"\n",
    "\n",
    "sample_string_embedding = st_model_small.encode(sample_string)\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [sample_string],\n",
    "    \"embedding\": [sample_string_embedding]\n",
    "})\n",
    "df"
   ],
   "id": "f6407144745c187a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why Use Embedding Vectors Instead of Strings?\n",
    "\n",
    "### üü¢ Drawbacks with Raw Strings\n",
    "- Computers see `\"dog\"` as just `\"d\", \"o\", \"g\"` ‚Äî no understanding of meaning\n",
    "- Hard to measure **similarity** or **relationships** between words\n",
    "- Not suitable for **search, clustering, or ML models**\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Benefits of Embeddings\n",
    "- **Numerical Representation:** Converts text into vectors that algorithms understand\n",
    "- **Capture Meaning:** Similar words or sentences are close in vector space\n",
    "- **Efficient & Scalable:** Enables fast similarity search with dot product or cosine similarity\n",
    "- **Handle Synonyms & Context:** `\"car\"` ‚âà `\"automobile\"`, context-aware models disambiguate `\"bank\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Example\n",
    "\n",
    "| Text      | Raw Representation         | Embedding (Example)          |\n",
    "|-----------|---------------------------|-----------------------------|\n",
    "| `\"dog\"`   | `\"d\", \"o\", \"g\"`           | `[0.12, -0.45, 0.87, ...]`  |\n",
    "| `\"puppy\"` | `\"p\", \"u\", \"p\", \"p\", \"y\"` | `[0.11, -0.48, 0.90, ...]`  |\n",
    "\n",
    "*Vectors are close ‚Üí words are semantically similar*\n"
   ],
   "id": "39d568b4144009af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üí° Uses Cases of Text Embedding Vectors\n",
    "\n",
    "| **Use Case**                   | **Example** |\n",
    "|--------------------------------|-------------|\n",
    "| **Semantic Search**         | Search ‚Äúdoctor‚Äù ‚Üí find content on ‚Äúphysicians‚Äù or ‚Äúhealthcare providers‚Äù even without exact keywords |\n",
    "| **Recommendation Systems**  | Suggest similar research papers, movies, or products based on descriptions or reviews |\n",
    "| **Sentiment Analysis**      | Understand tone (positive/negative/neutral) beyond simple keywords in tweets or reviews |\n",
    "| **Clustering & Topic Modeling** | Group thousands of news articles or support tickets by topic automatically |\n",
    "| **Chatbots & Virtual Assistants** | Improve NLU so bots answer contextually, not just by keyword |\n",
    "| **Fraud Detection**        | Spot unusual or suspicious text patterns in financial or insurance claims |\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "9a8378ed2096a374"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ü¶ñ Pre-Embedding Area\n",
    "üìå **Definition:**\n",
    "Early NLP approaches represented text as **sparse, high-dimensional vectors**.\n",
    "Each dimension corresponded to a **unique word or token**, with no sense of meaning or context.\n",
    "\n",
    "---\n",
    "\n",
    "### Bag-of-Words (BoW)\n",
    "- Represents documents as a **vector of word counts**\n",
    "- Ignores grammar, order, and semantics\n",
    "- Example:\n",
    "  `\"I like NLP\"` ‚Üí `[1, 1, 1, 0, 0, ...]`\n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)\n",
    "- Adjusts raw counts to emphasize **rare, informative words** and downweight common words\n",
    "- Example: ‚Äúthe‚Äù ‚Üí low weight, ‚Äúquantum‚Äù ‚Üí high weight\n",
    "\n",
    "---\n",
    "\n",
    "#### TF-IDF Formula\n",
    "\n",
    "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times \\log\\left(\\frac{N}{DF(t)}\\right)$$\n",
    "\n",
    "Where:\n",
    "- (TF(t, d)\\): Frequency of term \\(t\\) in document \\(d\\)\n",
    "- \\(DF(t)\\): Number of documents containing \\(t\\)\n",
    "- \\(N\\): Total number of documents\n"
   ],
   "id": "fca82183c30db9b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load parlamint dataset\n",
    "df_parlamint = pd.read_csv(\"../../../datasets/parlamint/parlamint-it-is-2022.txt\", sep=\"\\t\").head(2000)\n",
    "df_parlamint_subset = df_parlamint.head(1000).copy(deep=True)\n",
    "df_parlamint\n",
    "\n",
    "# Group sentence by utterance (=Parent_ID)\n",
    "df_parlamint_grouped = (df_parlamint.groupby([\"Parent_ID\"])[\"Text\"]\n",
    "                        .apply(lambda s: \" \".join(s))\n",
    "                        .reset_index(name=\"utterance_text\")).head(100)"
   ],
   "id": "da7a4f07c62bbc84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_parlamint.head(5)",
   "id": "a17ad6be73eb8281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_parlamint_grouped.head(5)",
   "id": "3559f6a50cab8ad3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Take a single utterance from the dataset\n",
    "sample_utterance = df_parlamint[df_parlamint[\"Parent_ID\"] == \"ParlaMint-IS_2022-01-17-20.u1\"][\"Text\"]\n",
    "print(\"\\n\".join([e for e in sample_utterance]))"
   ],
   "id": "b1b83d3a4ecbb120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from models import CountVectorizerEmbedder\n",
    "\n",
    "# Adding just the utterance sample as vocabulary\n",
    "cv_model = CountVectorizerEmbedder(vocabulary=sample_utterance, max_features=5, stop_words='english')\n",
    "# cv_model = CountVectorizerEmbedder(vocabulary=sample_utterance)\n",
    "\n",
    "cv_embeddings = cv_model.embed(sample_utterance)\n",
    "print(f\"Number features: {len(cv_model.embedding_model.get_feature_names_out())}\", cv_model.embedding_model.get_feature_names_out())\n",
    "print(f\"Shape embedding array: {cv_embeddings.toarray().shape}\")\n",
    "df_cv_output = pd.DataFrame(columns=cv_model.embedding_model.get_feature_names_out(), data=cv_embeddings.toarray())\n",
    "df_cv_output"
   ],
   "id": "8c5454306c1c7e25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from models import TfIdfEmbedder\n",
    "\n",
    "# Adding just the utterance sample as vocabulary\n",
    "tfidf_model = TfIdfEmbedder(vocabulary=sample_utterance, max_features=5, stop_words='english')\n",
    "tfidf_embeddings = tfidf_model.embed(sample_utterance)\n",
    "print(f\"Number features: {len(tfidf_model.embedding_model.get_feature_names_out())}\", tfidf_model.embedding_model.get_feature_names_out())\n",
    "print(f\"Shape embedding array: {tfidf_embeddings.toarray().shape}\")\n",
    "df_tfidf_output = pd.DataFrame(columns=tfidf_model.embedding_model.get_feature_names_out(), data=tfidf_embeddings.toarray())\n",
    "df_tfidf_output"
   ],
   "id": "3a257cc41078fcbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dense Text Embeddings\n",
    "\n",
    "üìå **Definition:**\n",
    "Dense embeddings represent words, sentences, or documents as **low-dimensional, dense vectors**\n",
    "where similar meanings are **close together in vector space**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Characteristics\n",
    "- **Low-dimensional** (e.g., 100‚Äì1,536 dimensions, not vocab-sized)\n",
    "- **Dense representation**: most values ‚â† 0\n",
    "- Captures **semantic meaning** and context\n",
    "- Learned from data via **neural networks**\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Brief History\n",
    "- **Word2Vec (2013)** ‚Äì First widely used dense word embeddings (Mikolov et al.)\n",
    "- **GloVe (2014)** ‚Äì Global Vectors for word representation\n",
    "- **FastText (2016)** ‚Äì Adds subword information for better handling of rare words\n",
    "- **ELMo (2018)** ‚Äì Contextual word embeddings\n",
    "- **BERT (2018)** ‚Äì Contextual embeddings for entire sentences\n",
    "- **OpenAI / Modern Embeddings (2020s)** ‚Äì High-quality sentence/document embeddings (e.g., `text-embedding-3-large`)\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Why It‚Äôs Better\n",
    "- Reduces dimensionality dramatically\n",
    "- Learns **semantic relationships**\n",
    "- Powers modern **search, recommendation, and AI assistants**\n"
   ],
   "id": "54a49210eb846dd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Word2Vec: CBOW & Skip-Gram Overview\n",
    "\n",
    "### üü¢ What is Word2Vec?\n",
    "- A **shallow, two-layer neural network** that learns word embeddings from context.\n",
    "- Maps words to **dense vectors** in a continuous space; similar words are **close together**.\n",
    "- Introduced by Mikolov et al., **2013**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü†Ô∏è Architectures\n",
    "\n",
    "#### üîπ Continuous Bag-of-Words (CBOW)\n",
    "- Predicts the **center word** given its context words.\n",
    "- Fast to train; works well for **frequent words**.\n",
    "\n",
    "#### üîπ Skip-Gram\n",
    "- Predicts **context words** given a center word.\n",
    "- Performs better for **rare words**, large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Why It Matters\n",
    "- Captures **semantic relationships**:\n",
    "  `king - man + woman ‚âà queen`\n",
    "- Major leap from sparse (BoW/TF-IDF) to **dense, meaningful embeddings**.\n"
   ],
   "id": "4df11a6497c4aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Word2Vec: Training & Visual Intuition\n",
    "\n",
    "### üü¢ Training Workflow\n",
    "1. **One-hot encoding** for words.\n",
    "2. **Hidden layer** = embedding lookup table.\n",
    "3. **Output layer** predicts context words (softmax with negative sampling).\n",
    "4. Final **hidden layer weights** = embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Visual Intuition\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "<img src=\"./images/word2vec_diagrams.png\" alt=\"Word2Vec Architecture\" style=\"width:100%;\">\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "  <img src=\"./images/skip_gram_net_arch.png\" alt=\"Word2Vec Context\" style=\"width:100%;\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- Left: Skip-Gram architecture predicting context words.\n",
    "- Right: Example of context window for a target word.\n",
    "- Only the **embedding layer weights** are retained.\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Israel G. (2017). *Word2Vec Explained*.\n",
    "  [https://israelg99.github.io/2017-03-23-Word2Vec-Explained/](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/)\n"
   ],
   "id": "4e4f2e288694c974"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BERT: Contextual Embeddings\n",
    "\n",
    "### üü¢ What is BERT?\n",
    "- **Bidirectional Encoder Representations from Transformers** (2018, Google AI).\n",
    "- Uses **Transformer architecture** to create **contextual word embeddings**:\n",
    "  - Each word‚Äôs vector depends on **all surrounding words** (left & right context).\n",
    "- Trained on **masked language modeling** and **next sentence prediction** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Key Innovations\n",
    "- **Bidirectional**: Unlike Word2Vec/Glove, captures context from both sides.\n",
    "- **Transformer encoder layers** with self-attention results in rich, deep embeddings.\n",
    "- **Contextualization**: Same word gets **different vectors** depending on context\n",
    "  (‚Äúbank‚Äù in ‚Äúriver bank‚Äù vs. ‚Äúbank account‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Visual Intuition\n",
    "\n",
    "<img src=\"images/bert_embeddings.png\" alt=\"BERT Transformer\" style=\"width:40%;\">\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Devlin et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.*\n",
    "  [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)\n"
   ],
   "id": "c018651f2f1655a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sentence Transformers Recap & Training\n",
    "\n",
    "### üü¢ Key Takeaways\n",
    "- **Sparse vectors** (BoW, TF-IDF):\n",
    "  - One dimension per word, mostly zeros\n",
    "  - No deep semantic meaning\n",
    "- **Dense embeddings** (Word2Vec, BERT, SBERT):\n",
    "  - Low-dimensional, rich semantic context\n",
    "  - Words/sentences cluster by meaning\n",
    "\n",
    "---\n",
    "\n",
    "### üü† How SBERT is Trained\n",
    "- **Backbone:** Pretrained BERT or RoBERTa encoders\n",
    "- **Siamese/Triplet Network Architecture:**\n",
    "  - Encodes two or three sentences **independently** into embeddings\n",
    "  - Trains to minimize distance for similar sentences and maximize for dissimilar ones\n",
    "- **Training Objectives [Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html):**\n",
    "  - **Contrastive loss** (distance-based similarity)\n",
    "  - **Natural Language Inference** datasets (entailment, contradiction, neutral)\n",
    "  - **MultipleNegativesRankingLoss** for retrieval tasks\n",
    "- **Result:**\n",
    "  - Embeddings suitable for **cosine similarity** ‚Üí semantic search, clustering, recommendations\n",
    "\n",
    "---\n",
    "\n",
    "### Reference\n",
    "- Reimers & Gurevych (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*\n",
    "  [https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)\n",
    "- SentenceTransformers Documentation [https://www.sbert.net/](https://www.sbert.net/)\n"
   ],
   "id": "4a753da191f6b7af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model_small = SentenceTransformer('all-minilm-l6-v2')\n",
    "\n",
    "sample_string = \"I really like this summer school!\"\n",
    "\n",
    "sample_string_embedding = st_model_small.encode(sample_string)\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [sample_string],\n",
    "    \"embedding\": [sample_string_embedding]\n",
    "})\n",
    "df"
   ],
   "id": "d325aa1e5d92a8c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Distance & Similarity Measures (Brief Intro)\n",
    "\n",
    "### üü¢ Why It Matters\n",
    "- Compare embeddings ‚Üí find **semantic similarity** between words, sentences, or documents.\n",
    "- Core to **semantic search, clustering, and topic modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Common Measures\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "üîπ <b>Dot Product</b>\n",
    "Unnormalized similarity; sensitive to magnitude:\n",
    "$$\n",
    "a \\cdot b = \\sum a_i b_i\n",
    "$$\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<img src=\"images/dot_prod.png\" alt=\"Dot Product\" width=\"200\" align=\"right\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "üîπ <b>Cosine Similarity</b>\n",
    "  Measures <b>angle</b> between vectors (ignores magnitude):\n",
    "  $$\n",
    "  \\text{cosine\\_sim}(a, b) = \\frac{a \\cdot b}{\\|a\\|\\|b\\|}\n",
    "  $$\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<img src=\"images/dot_prod.png\" alt=\"Dot Product\" width=\"200\" align=\"right\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ],
   "id": "3e3b73036e9aae95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Distance & Similarity Measures (Brief Intro)\n",
    "\n",
    "### üü† Common Measures cont'd\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "üîπ <b>Euclidean Distance</b>\n",
    "  Straight-line distance in vector space:\n",
    "  $$\n",
    "  d(a, b) = \\sqrt{\\sum (a_i - b_i)^2}\n",
    "  $$\n",
    " </td>\n",
    "<td width=\"30%\">\n",
    "<img src=\"images/euc_distance.png\" alt=\"Dot Product\" width=\"200\" align=\"right\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "üîπ <b>Manhattan (L1) Distance</b>\n",
    "  Sum of absolute differences:\n",
    "  $$\n",
    "  d_{\\text{L1}}(a, b) = \\sum |a_i - b_i|\n",
    "  $$\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<img src=\"images/manhattan_distance.png\" alt=\"Dot Product\" width=\"200\" align=\"right\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ],
   "id": "6cf7efebc7a2bb13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chunking Techniques for Embeddings\n",
    "\n",
    "### üü¢ Why Chunking?\n",
    "- Long documents exceed model token limits (e.g., BERT ~512 tokens).\n",
    "- Splitting text into **manageable chunks** improves:\n",
    "  - ‚úÖ Embedding quality\n",
    "  - ‚úÖ Retrieval accuracy\n",
    "  - ‚úÖ Context management in downstream tasks\n",
    "- üéØ **Goal of Good Chunking:**\n",
    "  Create chunks that are **small enough** to fit model limits\n",
    "  but **large enough** to retain full semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Common Techniques\n",
    "\n",
    "1. **Fixed-Length Chunking**\n",
    "   - Split text into chunks of `N` tokens/words.\n",
    "   - Simple, fast, but can cut off sentences mid-way.\n",
    "\n",
    "2. **Sentence-Based Chunking**\n",
    "   - Split by sentence boundaries (NLTK, spaCy).\n",
    "   - Better for readability, semantic grouping.\n",
    "\n",
    "3. **Paragraph-Based Chunking**\n",
    "   - Keep natural paragraph structure.\n",
    "   - Good for preserving context, but chunk sizes vary.\n",
    "\n",
    "4. **Sliding Window / Overlapping Chunks**\n",
    "   - Add overlap between chunks (e.g., 50 tokens).\n",
    "   - Prevents loss of context between splits.\n",
    "\n",
    "5. **Semantic Chunking**\n",
    "   - Use topic segmentation or embeddings to find boundaries.\n",
    "   - Most accurate, but computationally heavier.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Choose technique based on document length, model limits, and retrieval needs.**\n"
   ],
   "id": "fc120e13668e7cff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üèãÔ∏è Exercise: Simple Embedding Use Case",
   "id": "3e68f3c78877c9a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode utterance-wise dataset\n",
    "df_parlamint_embeddings_per_utterance = st_model_small.encode(df_parlamint_grouped[\"utterance_text\"].to_list(),\n",
    "                                                     show_progress_bar=True)\n",
    "\n",
    "# Encode sentence-wise dataset\n",
    "df_parlamint_embeddings_per_sentence = st_model_small.encode(df_parlamint[\"Text\"].to_list(), show_progress_bar=True)"
   ],
   "id": "8724f653f76318c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_parlamint_grouped[\"embedding\"] = list(df_parlamint_embeddings_per_utterance)\n",
    "df_parlamint[\"embedding\"] = list(df_parlamint_embeddings_per_sentence)\n",
    "df_parlamint"
   ],
   "id": "fbd8ed9658c8857f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "\n",
    "question = \"What is the government policy on climate change?\"\n",
    "# question = \"America?\"\n",
    "k = 5  # choose how many results you want\n",
    "\n",
    "# 1. Embed the question\n",
    "question_embedding = st_model_small.encode(question)\n",
    "\n",
    "# 2. Compute cosine similarities\n",
    "cosine_similarities = util.cos_sim(question_embedding, df_parlamint_embeddings_per_utterance)[0].cpu().numpy()\n",
    "\n",
    "# 3. Get indices of top-k most similar utterances\n",
    "top_k_idx = np.argsort(cosine_similarities)[::-1][:k]\n",
    "\n",
    "# 4. Retrieve the top-k utterances and their similarity scores\n",
    "for idx in top_k_idx:\n",
    "    text = df_parlamint_grouped.iloc[idx][\"utterance_text\"]\n",
    "    score = cosine_similarities[idx]\n",
    "    print(f\"Score: {score:.4f} | Utterance: {text}\\n\")\n"
   ],
   "id": "b0d5bca3d1b87c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding It All Together\n",
    "\n",
    "### üü¢ The Journey So Far\n",
    "- **Sparse Vectors (BoW, TF-IDF):**\n",
    "  High-dimensional, simple counts, no semantics\n",
    "- **Dense Word Embeddings (Word2Vec, GloVe):**\n",
    "  Compact vectors capturing basic word meaning\n",
    "- **Contextual Models (BERT):**\n",
    "  Token embeddings adapt to context\n",
    "- **Sentence Transformers (SBERT):**\n",
    "  Sentence-level semantic embeddings for similarity & search\n",
    "- **Chunking Strategies:**\n",
    "  Break long texts into meaningful, model-friendly pieces\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Why It Matters\n",
    "- Transform raw text into **meaningful numerical representations**\n",
    "- Enable **semantic search, clustering, Q&A, recommendations**\n",
    "- Foundation for **modern NLP pipelines & AI assistants**\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Key Takeaway\n",
    "A well-designed embedding pipeline =\n",
    "**Chunking + Contextual Models + Smart Similarity Metrics**\n",
    "‚Üí Powerful, scalable text understanding!\n"
   ],
   "id": "d8bc9cacd84b50b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üèãÔ∏è Exercises: Hands-On with Embeddings\n",
    "\n",
    "### üü† Generate embeddings for the Parlamint (sub) dataset\n",
    "- **Experiment with different embedding techniques:**\n",
    "  - Compare different embedding techniques (dense vs sparse) regarding (i) vector dimensionality (ii) Semantic similarity (are similar texts actually closer together?)\n",
    "  - Suggested algorithms: [BERTopic Models](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html) or [HuggingFace (general)](https://huggingface.co/models?other=embeddings&sort=trending) or [Huggingface (sentence transformers)](https://huggingface.co/sentence-transformers/models)\n",
    "- **Consider different chunking techniques:**\n",
    "  - Sentence based vs utterance level vs ??\n",
    "- **Save them as pickle file(s):** `df_dataset.to_pickle(\"<path_and_filename>.pkl\")`\n",
    "\n",
    "### üü† Generate embeddings for the HSA (sub) dataset\n",
    "- **Same tasks as for the Parlamint dataset**\n",
    "\n",
    "### üü† Retrieval: Play around with embeddings and similarity retrieval\n",
    "- **Write queries:**\n",
    "  - Search for valid topics, write queries and manually evaluate the result\n",
    "- **Consider different chunking techniques:**\n",
    "  - Sentence based vs utterance level\n",
    "  - Are topics semantically better captured on sentence level or utterance level=\n",
    "- **Utilize different embedding models for retrieval:**\n",
    "  - Sparse vs dense embeddings\n",
    "  - Experiment with different similarity scores"
   ],
   "id": "3ab68c113b02712"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
